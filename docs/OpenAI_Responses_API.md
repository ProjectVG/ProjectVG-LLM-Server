# OpenAI Responses API ì™„ì „ ê°€ì´ë“œ

## ê°œìš”
OpenAI Responses APIëŠ” 2025ë…„ 3ì›”ì— ì¶œì‹œëœ ê°€ì¥ ì§„ë³´ëœ APIë¡œ, Chat Completions APIì™€ Assistants APIì˜ ì¥ì ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ê²°í•©í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ê´€ë¦¬(stateful), ë‚´ì¥ ë„êµ¬, ì›¹ ê²€ìƒ‰, íŒŒì¼ ê²€ìƒ‰ ë“± ê³ ê¸‰ ê¸°ëŠ¥ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## ê¸°ë³¸ ì •ë³´

**ì—”ë“œí¬ì¸íŠ¸:** `POST https://api.openai.com/v1/responses`  
**ì¸ì¦:** Bearer Token ë°©ì‹  
**íŠ¹ì§•:** Chat Completionsì˜ ëª¨ë“  ê¸°ëŠ¥ + ì¶”ê°€ ê³ ê¸‰ ê¸°ëŠ¥

## ì™„ì „ íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

### í•µì‹¬ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | í•„ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|------|------|-------|------|
| `model` | string | âœ“ | - | ì‚¬ìš©í•  AI ëª¨ë¸ |
| `input` | string/array | âœ“ | - | í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, íŒŒì¼ ì…ë ¥ |
| `instructions` | string/null | | null | ì‹œìŠ¤í…œ(ê°œë°œì) ë©”ì‹œì§€ |
| `max_output_tokens` | integer/null | | null | ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ |
| `temperature` | number/null | | 1 | ìƒ˜í”Œë§ ì˜¨ë„ (0-2) |
| `top_p` | number/null | | 1 | í•µì‹¬ ìƒ˜í”Œë§ í™•ë¥  |

### ê³ ê¸‰ ê¸°ëŠ¥ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|------|-------|------|
| `background` | boolean/null | false | ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì—¬ë¶€ |
| `conversation` | string/object/null | null | ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ |
| `previous_response_id` | string/null | null | ì´ì „ ì‘ë‹µ ID (ë©€í‹°í„´ ëŒ€í™”) |
| `store` | boolean/null | true | ì‘ë‹µ ì €ì¥ ì—¬ë¶€ |
| `stream` | boolean/null | false | ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€ |
| `parallel_tool_calls` | boolean/null | true | ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ í—ˆìš© |
| `max_tool_calls` | integer/null | null | ìµœëŒ€ ë„êµ¬ í˜¸ì¶œ ìˆ˜ |

### ë„êµ¬ ë° ì¶œë ¥ ì˜µì…˜

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ì„¤ëª… |
|---------|------|------|
| `tools` | array | ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë°°ì—´ |
| `tool_choice` | string/object | ë„êµ¬ ì„ íƒ ë°©ì‹ |
| `include` | array/null | ì¶”ê°€ ì¶œë ¥ ë°ì´í„° í¬í•¨ ì˜µì…˜ |
| `text` | object | í…ìŠ¤íŠ¸ ì‘ë‹µ êµ¬ì„± ì˜µì…˜ |
| `reasoning` | object/null | ì¶”ë¡  ëª¨ë¸ ì„¤ì • (o-ì‹œë¦¬ì¦ˆ ì „ìš©) |

### ë©”íƒ€ë°ì´í„° ë° ë³´ì•ˆ

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ì„¤ëª… |
|---------|------|------|
| `metadata` | map | í‚¤-ê°’ ìŒ ë©”íƒ€ë°ì´í„° (16ê°œ ì œí•œ) |
| `safety_identifier` | string | ì‚¬ìš©ì ì‹ë³„ì (ì •ì±… ìœ„ë°˜ ê°ì§€) |
| `prompt_cache_key` | string | ìºì‹œ ìµœì í™” í‚¤ |
| `service_tier` | string/null | ì²˜ë¦¬ ê³„ì¸µ ('auto', 'default', 'flex', 'priority') |
| `truncation` | string/null | ì»¨í…ìŠ¤íŠ¸ ì°½ ì´ˆê³¼ ì‹œ ì²˜ë¦¬ ë°©ì‹ |

### include ì˜µì…˜ ìƒì„¸

```json
{
  "include": [
    "web_search_call.action.sources",
    "code_interpreter_call.outputs", 
    "computer_call_output.output.image_url",
    "file_search_call.results",
    "message.input_image.image_url",
    "message.output_text.logprobs",
    "reasoning.encrypted_content"
  ]
}
```

## ì…ë ¥ í˜•ì‹ ìƒì„¸

### 1. í…ìŠ¤íŠ¸ ì…ë ¥
```python
# ê°„ë‹¨í•œ ë¬¸ìì—´
response = client.responses.create(
    model="gpt-4o-mini",
    input="tell me a joke"
)

# ë©”ì‹œì§€ ë°°ì—´ í˜•ì‹
response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."}
    ]
)
```

### 2. ì´ë¯¸ì§€ ì…ë ¥
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?"},
                {"type": "input_image", "image_url": "https://example.com/image.jpg"}
            ]
        }
    ]
)
```

### 3. íŒŒì¼ ì…ë ¥
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user", 
            "content": [
                {"type": "input_text", "text": "ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                {"type": "input_file", "file_id": "file-abc123"}
            ]
        }
    ]
)
```

## ì‘ë‹µ í˜•ì‹

### ì„±ê³µ ì‘ë‹µ
```json
{
  "id": "resp_abc123",
  "object": "response",
  "created_at": 1234567890,
  "model": "gpt-4o-mini",
  "output_text": "AI ì‘ë‹µ í…ìŠ¤íŠ¸",
  "temperature": 0.7,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "usage": {
    "input_tokens": 50,
    "output_tokens": 100,
    "total_tokens": 150
  },
  "status": "completed"
}
```

### ì‘ë‹µ í•„ë“œ ì„¤ëª…

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `id` | string | ì‘ë‹µì˜ ê³ ìœ  ì‹ë³„ì |
| `object` | string | ê°ì²´ íƒ€ì… ("response") |
| `created_at` | integer | Unix íƒ€ì„ìŠ¤íƒ¬í”„ |
| `model` | string | ì‚¬ìš©ëœ ëª¨ë¸ëª… |
| `output_text` | string | AIê°€ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸ |
| `temperature` | number | ì‚¬ìš©ëœ temperature ê°’ |
| `text.format.type` | string | í…ìŠ¤íŠ¸ í˜•ì‹ íƒ€ì… |
| `usage.input_tokens` | integer | ì…ë ¥ì— ì‚¬ìš©ëœ í† í° ìˆ˜ |
| `usage.output_tokens` | integer | ì¶œë ¥ì— ì‚¬ìš©ëœ í† í° ìˆ˜ |
| `usage.total_tokens` | integer | ì´ ì‚¬ìš©ëœ í† í° ìˆ˜ |
| `status` | string | ì‘ë‹µ ìƒíƒœ |

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‘ë‹µ
```python
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê°„ë‹¨í•œ ì˜ˆì‹œ
response = client.responses.create(
    model="gpt-4o-mini",
    input="Write a one-sentence bedtime story about a unicorn."
)
print(response.output_text)

# ìƒì„¸ ì„¤ì • ì˜ˆì‹œ
response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?"}
    ],
    instructions="ê°„ë‹¨í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
    temperature=0.7,
    max_output_tokens=1000,
    metadata={"user_id": "user_123", "session": "session_456"}
)
print(response.output_text)
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
```python
stream = client.responses.create(
    model="gpt-4o",
    input="Say 'double bubble bath' ten times fast.",
    stream=True,
)

print("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:")
for chunk in stream:
    if chunk.output:
        for output in chunk.output:
            if hasattr(output, 'content') and output.content:
                print(output.content, end='', flush=True)
print()
```

### 3. ëŒ€í™” ìƒíƒœ ê´€ë¦¬
```python
# ìˆ˜ë™ ëŒ€í™” ê´€ë¦¬
history = [{"role": "user", "content": "tell me a joke"}]

response = client.responses.create(
    model="gpt-4o-mini", 
    input=history,
    store=False
)

# íˆìŠ¤í† ë¦¬ì— ì‘ë‹µ ì¶”ê°€
history.extend([
    {"role": "assistant", "content": response.output_text},
    {"role": "user", "content": "tell me another one"}
])

# ë‹¤ìŒ ì‘ë‹µ
response2 = client.responses.create(
    model="gpt-4o-mini",
    input=history,
    store=False
)

# ìë™ ëŒ€í™” ê´€ë¦¬ (conversation ì‚¬ìš©)
response1 = client.responses.create(
    model="gpt-4o-mini",
    input="knock knock.",
    conversation="conv_123"
)

response2 = client.responses.create(
    model="gpt-4o-mini", 
    input="Orange.",
    conversation="conv_123"
)
```

### 4. ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì˜ˆì‹œ
```python
import time

# ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘
response = client.responses.create(
    model="o3",
    input="Write a comprehensive analysis of quantum computing",
    background=True,
    max_output_tokens=4000
)

response_id = response.id
print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘: {response_id}")

# ìƒíƒœ í™•ì¸ ë£¨í”„
while True:
    status = client.responses.retrieve(response_id)
    print(f"ìƒíƒœ: {status.status}")
    
    if status.status in ["completed", "failed", "cancelled"]:
        break
    
    time.sleep(5)  # 5ì´ˆë§ˆë‹¤ í™•ì¸

if status.status == "completed":
    print("ì™„ë£Œëœ ì‘ë‹µ:")
    print(status.output_text)
else:
    print(f"ì‘ì—… ì‹¤íŒ¨ ë˜ëŠ” ì·¨ì†Œ: {status.status}")
```

### 5. ì›¹ ê²€ìƒ‰ í™œìš©
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input="2025ë…„ í•œêµ­ì˜ ìµœì‹  AI ê¸°ìˆ  ë™í–¥ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    tools=[{"type": "web_search"}],
    include=["web_search_call.action.sources"]
)

print("ì‘ë‹µ:", response.output_text)

# ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤ ì •ë³´ í™•ì¸
if hasattr(response, 'web_search_sources'):
    print("\nì°¸ê³  ì†ŒìŠ¤:")
    for source in response.web_search_sources:
        print(f"- {source}")
```

### 6. íŒŒì¼ ê²€ìƒ‰ í™œìš©
```python
# ë¨¼ì € íŒŒì¼ ì—…ë¡œë“œ
file = client.files.create(
    file=open("document.pdf", "rb"),
    purpose="assistants"
)

# íŒŒì¼ ê²€ìƒ‰ì„ í†µí•œ ì‘ë‹µ
response = client.responses.create(
    model="gpt-4o-mini",
    input="ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    tools=[{"type": "file_search", "file_search": {"vector_store_ids": [file.id]}}],
    include=["file_search_call.results"]
)
```

### 7. Azure OpenAI ì‚¬ìš©ë²•
```python
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

token_provider = get_bearer_token_provider(
    DefaultAzureCredential(), 
    "https://cognitiveservices.azure.com/.default"
)

client = AzureOpenAI(
    azure_endpoint="https://YOUR-RESOURCE-NAME.openai.azure.com/",
    azure_ad_token_provider=token_provider,
    api_version="2024-10-21"
)

response = client.responses.create(
    model="gpt-4o-mini",  # Azure deployment name
    input="Hello from Azure!"
)
```

## ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬

### ìƒíƒœ ì¡°íšŒ
**ì—”ë“œí¬ì¸íŠ¸:** `GET /v1/responses/{id}`

### ì·¨ì†Œ ìš”ì²­  
**ì—”ë“œí¬ì¸íŠ¸:** `POST /v1/responses/{id}/cancel`

### ìƒíƒœ ê°’
- `queued`: ëŒ€ê¸° ì¤‘
- `in_progress`: ì²˜ë¦¬ ì¤‘  
- `completed`: ì™„ë£Œ
- `failed`: ì‹¤íŒ¨
- `cancelled`: ì·¨ì†Œë¨

## ì˜¤ë¥˜ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì½”ë“œ
- `400`: ì˜ëª»ëœ ìš”ì²­
- `401`: ì¸ì¦ ì‹¤íŒ¨
- `429`: ìš”ì²­ ì œí•œ ì´ˆê³¼
- `500`: ì„œë²„ ì˜¤ë¥˜

### ì˜¤ë¥˜ ì‘ë‹µ ì˜ˆì‹œ
```json
{
  "error": {
    "message": "Invalid API key provided",
    "type": "invalid_request_error",
    "code": "invalid_api_key"
  }
}
```

## ì£¼ì˜ì‚¬í•­

1. **í† í° ì œí•œ**: ëª¨ë¸ë³„ë¡œ ìµœëŒ€ í† í° ìˆ˜ê°€ ë‹¤ë¦„
2. **ìš”ì²­ ì œí•œ**: API í‚¤ë³„ ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ ì¡´ì¬
3. **ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬**: ê¸´ ì‘ì—…ì€ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ê¶Œì¥
4. **ë¹„ìš©**: ì‚¬ìš©í•œ í† í° ìˆ˜ì— ë”°ë¼ ê³¼ê¸ˆ

## ì§€ì› ëª¨ë¸

- `gpt-4o-mini`: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸
- `gpt-4o`: ê³ ì„±ëŠ¥ ë²”ìš© ëª¨ë¸
- `o3`: ìµœì‹  ê³ ê¸‰ ëª¨ë¸ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì§€ì›)

## ë‚´ì¥ ë„êµ¬ (Built-in Tools)

### ì›¹ ê²€ìƒ‰ (Web Search)
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input="ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
    tools=[{"type": "web_search"}],
    include=["web_search_call.action.sources"]
)
```

### íŒŒì¼ ê²€ìƒ‰ (File Search) 
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input="ë¬¸ì„œì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
    tools=[{
        "type": "file_search",
        "file_search": {"vector_store_ids": ["vs_123"]}
    }],
    include=["file_search_call.results"]
)
```

### ì½”ë“œ ì¸í„°í”„ë¦¬í„° (Code Interpreter)
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input="ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”",
    tools=[{"type": "code_interpreter"}],
    include=["code_interpreter_call.outputs"]
)
```

### ì»´í“¨í„° ì‚¬ìš© (Computer Use - Preview)
```python
response = client.responses.create(
    model="computer-use-preview",
    input="ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  íŠ¹ì • ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ì£¼ì„¸ìš”",
    tools=[{"type": "computer"}],
    include=["computer_call_output.output.image_url"]
)
```

## ê³ ê¸‰ ê¸°ëŠ¥

### 1. ì¶”ë¡  ëª¨ë¸ ì„¤ì • (o-ì‹œë¦¬ì¦ˆ)
```python
response = client.responses.create(
    model="o3-mini",
    input="ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”",
    reasoning={
        "effort": "high",  # low, medium, high
        "max_reasoning_tokens": 50000
    },
    include=["reasoning.encrypted_content"]
)
```

### 2. êµ¬ì¡°í™”ëœ ì¶œë ¥
```python
response = client.responses.create(
    model="gpt-4o-mini",
    input="ì‚¬ìš©ì ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”",
    text={
        "format": {
            "type": "json_schema",
            "json_schema": {
                "name": "user_info",
                "schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "age": {"type": "integer"},
                        "email": {"type": "string"}
                    }
                }
            }
        }
    }
)
```

### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```python
response = client.responses.create(
    model="gpt-4o-mini",
    prompt={
        "template_id": "template_123",
        "variables": {
            "user_name": "í™ê¸¸ë™",
            "topic": "AI ê¸°ìˆ "
        }
    }
)
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ëª¨ë²” ì‚¬ë¡€

### ì—ëŸ¬ í•¸ë“¤ë§
```python
from openai import OpenAI
import openai

client = OpenAI()

try:
    response = client.responses.create(
        model="gpt-4o-mini",
        input="Hello world",
        max_output_tokens=1000
    )
    print(response.output_text)
    
except openai.RateLimitError as e:
    print(f"Rate limit ì´ˆê³¼: {e}")
    # ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
    
except openai.APIError as e:
    print(f"API ì˜¤ë¥˜: {e}")
    
except Exception as e:
    print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
```

### ë¹„ìš© ìµœì í™” íŒ
```python
# 1. ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
response = client.responses.create(
    model="gpt-4o-mini",  # ë¹„ìš© íš¨ìœ¨ì 
    input="Simple question",
    max_output_tokens=100  # í† í° ì œí•œ
)

# 2. ìºì‹œ í™œìš©
response = client.responses.create(
    model="gpt-4o-mini",
    input="Repeated question",
    prompt_cache_key="cache_key_123"  # ìºì‹œ í‚¤ ì‚¬ìš©
)

# 3. ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë¡œ ë³‘ë ¬ ì‘ì—…
tasks = [] 
for query in queries:
    task = client.responses.create(
        model="gpt-4o-mini",
        input=query,
        background=True
    )
    tasks.append(task.id)
```

## ì„±ëŠ¥ ìµœì í™”

### ë°°ì¹˜ ì²˜ë¦¬
```python
import asyncio
from openai import AsyncOpenAI

async_client = AsyncOpenAI()

async def process_multiple_requests():
    tasks = []
    
    for i in range(10):
        task = async_client.responses.create(
            model="gpt-4o-mini",
            input=f"Query {i}",
        )
        tasks.append(task)
    
    responses = await asyncio.gather(*tasks)
    return responses

# ì‹¤í–‰
responses = asyncio.run(process_multiple_requests())
```

## Responses API vs Chat Completions ë¹„êµ

### Responses APIì˜ ì¥ì 

| ê¸°ëŠ¥ | Chat Completions | Responses API |
|------|------------------|---------------|
| **ìƒíƒœ ê´€ë¦¬** | ìˆ˜ë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ | ìë™ ìƒíƒœ ê´€ë¦¬ (`previous_response_id`) |
| **ë‚´ì¥ ë„êµ¬** | ìˆ˜ë™ êµ¬í˜„ í•„ìš” | ë‚´ì¥ ì›¹ê²€ìƒ‰, íŒŒì¼ê²€ìƒ‰, ì½”ë“œì‹¤í–‰ ë“± |
| **ì¶”ë¡  ì„±ëŠ¥** | ê¸°ë³¸ ì„±ëŠ¥ | 3% í–¥ìƒ (SWE-benchmark) |
| **ë¹„ìš© íš¨ìœ¨ì„±** | ê¸°ë³¸ ë¹„ìš© | 40-80% ë¹„ìš© ì ˆê° (ìºì‹œ ìµœì í™”) |
| **ì•”í˜¸í™” ì¶”ë¡ ** | ì§€ì› ì•ˆí•¨ | ZDR ì¡°ì§ì„ ìœ„í•œ ì•”í˜¸í™” ì¶”ë¡  ì§€ì› |
| **ì´ë²¤íŠ¸ ê¸°ë°˜** | í† í°ë³„ ìŠ¤íŠ¸ë¦¬ë° | ì˜ë¯¸ë¡ ì  ì´ë²¤íŠ¸ ê¸°ë°˜ |
| **ì…ë ¥ í˜•ì‹** | `messages` ë°°ì—´ë§Œ | ë¬¸ìì—´ ë˜ëŠ” ë°°ì—´ (`input`) |

### ê¸°ëŠ¥ë³„ ë¹„êµí‘œ

| ê¸°ëŠ¥ | Chat Completions | Responses API |
|------|------------------|---------------|
| í…ìŠ¤íŠ¸ ìƒì„± | âœ… | âœ… |
| ì˜¤ë””ì˜¤ | âœ… | ğŸš§ ê³§ ì¶œì‹œ |
| ë¹„ì „ | âœ… | âœ… |
| êµ¬ì¡°í™”ëœ ì¶œë ¥ | âœ… | âœ… |
| í•¨ìˆ˜ í˜¸ì¶œ | âœ… | âœ… |
| ì›¹ ê²€ìƒ‰ | âŒ | âœ… |
| íŒŒì¼ ê²€ìƒ‰ | âŒ | âœ… |
| ì»´í“¨í„° ì‚¬ìš© | âŒ | âœ… |
| ì½”ë“œ ì¸í„°í”„ë¦¬í„° | âŒ | âœ… |
| MCP | âŒ | âœ… |
| ì´ë¯¸ì§€ ìƒì„± | âŒ | âœ… |
| ì¶”ë¡  ìš”ì•½ | âŒ | âœ… |

## ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 1ë‹¨ê³„: ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ë§ˆì´ê·¸ë ˆì´ì…˜

**Chat Completions (ì´ì „)**
```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-5",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
print(completion.choices[0].message.content)
```

**Responses API (ì´í›„)**
```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    instructions="You are a helpful assistant.",
    input="Hello!"
)
print(response.output_text)
```

**ë˜ëŠ” ë©”ì‹œì§€ ë°°ì—´ í˜•ì‹**
```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
print(response.output_text)
```

### 2ë‹¨ê³„: ë©€í‹°í„´ ëŒ€í™” ë§ˆì´ê·¸ë ˆì´ì…˜

**Chat Completions (ìˆ˜ë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬)**
```python
# ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ê´€ë¦¬í•´ì•¼ í•¨
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"}
]

res1 = client.chat.completions.create(model="gpt-5", messages=messages)

# ìˆ˜ë™ìœ¼ë¡œ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
messages.append({"role": "assistant", "content": res1.choices[0].message.content})
messages.append({"role": "user", "content": "And its population?"})

res2 = client.chat.completions.create(model="gpt-5", messages=messages)
```

**Responses API (ìë™ ìƒíƒœ ê´€ë¦¬)**
```python
# ì²« ë²ˆì§¸ ìš”ì²­
res1 = client.responses.create(
    model="gpt-5",
    input="What is the capital of France?",
    store=True  # ìƒíƒœ ì €ì¥ í™œì„±í™”
)

# ë‘ ë²ˆì§¸ ìš”ì²­ - previous_response_idë¡œ ì»¨í…ìŠ¤íŠ¸ ìë™ ì—°ê²°
res2 = client.responses.create(
    model="gpt-5",
    input="And its population?",
    previous_response_id=res1.id,  # ì´ì „ ì‘ë‹µ ì°¸ì¡°
    store=True
)
```

**ZDR(Zero Data Retention) ì¡°ì§ìš© ì•”í˜¸í™” ì¶”ë¡ **
```python
# ë°ì´í„° ë³´ê´€ ì œí•œì´ ìˆëŠ” ì¡°ì§ìš©
res1 = client.responses.create(
    model="gpt-5",
    input="What is the capital of France?",
    store=False,  # ì €ì¥ ë¹„í™œì„±í™”
    include=["reasoning.encrypted_content"]  # ì•”í˜¸í™”ëœ ì¶”ë¡  í¬í•¨
)

res2 = client.responses.create(
    model="gpt-5",
    input="And its population?",
    store=False,
    include=["reasoning.encrypted_content"],
    # ì•”í˜¸í™”ëœ ì¶”ë¡  ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬ (êµ¬ì²´ì  êµ¬í˜„ì€ ë¬¸ì„œ ì°¸ì¡°)
)
```

### 3ë‹¨ê³„: ë„êµ¬ ì‚¬ìš© ë§ˆì´ê·¸ë ˆì´ì…˜

**Chat Completions (ìˆ˜ë™ ë„êµ¬ êµ¬í˜„)**
```python
import requests

def web_search(query):
    # ìì²´ ì›¹ ê²€ìƒ‰ API êµ¬í˜„ í•„ìš”
    r = requests.get(f"https://api.example.com/search?q={query}")
    return r.json().get("results", [])

completion = client.chat.completions.create(
    model="gpt-5",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who is the current president of France?"}
    ],
    functions=[
        {
            "name": "web_search",
            "description": "Search the web for information",
            "parameters": {
                "type": "object",
                "properties": {"query": {"type": "string"}},
                "required": ["query"]
            }
        }
    ]
)
```

**Responses API (ë‚´ì¥ ë„êµ¬ í™œìš©)**
```python
# ë‚´ì¥ ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© - êµ¬í˜„ ë¶ˆí•„ìš”!
answer = client.responses.create(
    model="gpt-5",
    input="Who is the current president of France?",
    tools=[{"type": "web_search_preview"}],
    include=["web_search_call.action.sources"]  # ê²€ìƒ‰ ì†ŒìŠ¤ í¬í•¨
)
print(answer.output_text)
```

### 4ë‹¨ê³„: êµ¬ì¡°í™”ëœ ì¶œë ¥ ë§ˆì´ê·¸ë ˆì´ì…˜

**Chat Completions**
```python
completion = client.chat.completions.create(
    model="gpt-5",
    messages=[{"role": "user", "content": "Extract user info as JSON"}],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "user_info",
            "schema": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "age": {"type": "integer"}
                }
            }
        }
    }
)
```

**Responses API**
```python
response = client.responses.create(
    model="gpt-5",
    input="Extract user info as JSON",
    text={
        "format": {
            "type": "json_schema",
            "json_schema": {
                "name": "user_info",
                "schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "age": {"type": "integer"}
                    }
                }
            }
        }
    }
)
```

### 5ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ë§ˆì´ê·¸ë ˆì´ì…˜

**Chat Completions (í† í°ë³„ ìŠ¤íŠ¸ë¦¬ë°)**
```python
stream = client.chat.completions.create(
    model="gpt-5",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

**Responses API (ì˜ë¯¸ë¡ ì  ì´ë²¤íŠ¸ ê¸°ë°˜)**
```python
stream = client.responses.create(
    model="gpt-5",
    input="Tell me a story",
    stream=True
)

for event in stream:
    if event.output:
        for output in event.output:
            if hasattr(output, 'content') and output.content:
                print(output.content, end="", flush=True)
```

## ì‘ë‹µ êµ¬ì¡° ë¹„êµ

### Chat Completions ì‘ë‹µ
```json
{
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ]
}
```

### Responses API ì‘ë‹µ
```json
{
  "id": "msg_67b73f697ba4819183a15cc17d011509",
  "output": [
    {
      "id": "msg_67b73f697ba4819183a15cc17d011509",
      "type": "message",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
          "annotations": []
        }
      ]
    }
  ],
  "output_text": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
}
```

## ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ê¸°ë³¸ ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] `messages` â†’ `input` ë³€ê²½
- [ ] `max_tokens` â†’ `max_output_tokens` ë³€ê²½
- [ ] `response.choices[0].message.content` â†’ `response.output_text` ë³€ê²½
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ì—…ë°ì´íŠ¸

### âœ… ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©
- [ ] `previous_response_id`ë¥¼ ì‚¬ìš©í•œ ë©€í‹°í„´ ëŒ€í™” êµ¬í˜„
- [ ] ë‚´ì¥ ë„êµ¬(`web_search_preview`, `file_search` ë“±) ë„ì…
- [ ] `instructions` í•„ë“œë¡œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¶„ë¦¬
- [ ] `include` í•„ë“œë¡œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ìš”ì²­

### âœ… ì„±ëŠ¥ ìµœì í™”
- [ ] `store: true`ë¡œ ìƒíƒœ ê´€ë¦¬ í™œì„±í™” (ë¹„ìš© ì ˆê°)
- [ ] `prompt_cache_key`ë¡œ ìºì‹œ ìµœì í™”
- [ ] ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ (`background: true`) ë„ì…

### âœ… ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤
- [ ] ZDR ì¡°ì§: `store: false` + `reasoning.encrypted_content` ì„¤ì •
- [ ] `safety_identifier`ë¡œ ì‚¬ìš©ì ì¶”ì  êµ¬í˜„
- [ ] `metadata`ë¡œ ìš”ì²­ íƒœê¹…

## ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

Responses APIëŠ” Chat Completionsì˜ ìƒìœ„ì§‘í•©ì´ë¯€ë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:

1. **1ë‹¨ê³„**: ìƒˆ ê¸°ëŠ¥ë¶€í„° Responses API ì‚¬ìš©
2. **2ë‹¨ê³„**: ì¶”ë¡  ëª¨ë¸ì´ í•„ìš”í•œ ì›Œí¬í”Œë¡œìš° ë§ˆì´ê·¸ë ˆì´ì…˜
3. **3ë‹¨ê³„**: ë¹„ìš© ì ˆê°ì´ ì¤‘ìš”í•œ ê³ ë¹ˆë„ API í˜¸ì¶œ ë§ˆì´ê·¸ë ˆì´ì…˜
4. **4ë‹¨ê³„**: ì „ì²´ ì‹œìŠ¤í…œ ë§ˆì´ê·¸ë ˆì´ì…˜

## Assistants APIì—ì„œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜

2025ë…„ 8ì›” 26ì¼ë¶€í„° Assistants APIê°€ deprecateë˜ë©°, 2026ë…„ 8ì›” 26ì¼ì— ì™„ì „ ì¢…ë£Œë©ë‹ˆë‹¤. Responses APIê°€ Assistants APIì˜ ê°œì„ ëœ ë²„ì „ì…ë‹ˆë‹¤.

**ì£¼ìš” ê°œì„ ì‚¬í•­:**
- ë” ë¹ ë¥¸ ì‘ë‹µ ì†ë„
- ë” ìœ ì—°í•œ API êµ¬ì¡°  
- ë” ë‚˜ì€ íƒ€ì… ì•ˆì „ì„±
- í†µí•©ëœ ë„êµ¬ ìƒíƒœê³„

## ì§€ì› ëª¨ë¸ ë° ê°€ê²©

### ì§€ì› ëª¨ë¸
- **gpt-4o**: ê³ ì„±ëŠ¥ ë²”ìš© ëª¨ë¸
- **gpt-4o-mini**: ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì   
- **o3**, **o3-mini**: ìµœì‹  ì¶”ë¡  ëª¨ë¸
- **computer-use-preview**: ì»´í“¨í„° ì œì–´ ê¸°ëŠ¥

### íŠ¹ìˆ˜ ê¸°ëŠ¥ë³„ ëª¨ë¸ ì§€ì›
| ê¸°ëŠ¥ | ì§€ì› ëª¨ë¸ |
|------|----------|
| ì›¹ ê²€ìƒ‰ | gpt-4o, gpt-4o-mini |
| íŒŒì¼ ê²€ìƒ‰ | ëª¨ë“  ëª¨ë¸ |
| ì½”ë“œ ì‹¤í–‰ | gpt-4o, gpt-4o-mini |
| ì¶”ë¡  ê¸°ëŠ¥ | o3, o3-mini |
| ì»´í“¨í„° ì œì–´ | computer-use-preview |

---

**ì°¸ê³  ë¬¸í—Œ:**
- [OpenAI Platform API Reference](https://platform.openai.com/docs/api-reference/responses)
- [OpenAI Cookbook - Responses API Examples](https://cookbook.openai.com/examples/responses_api/responses_example)
- [Azure OpenAI Responses API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses)
- [OpenAI Responses API Guide - DataCamp](https://www.datacamp.com/tutorial/openai-responses-api)
- [Official OpenAI Python Library](https://github.com/openai/openai-python)