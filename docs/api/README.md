# LLM Server API ë¬¸ì„œ

ì´ ë¬¸ì„œëŠ” LLM Serverì˜ ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ ìƒì„¸í•œ ëª…ì„¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‹ API ê°œìš”

- **Base URL**: `http://localhost:5601`
- **API Version**: v1
- **Content-Type**: `application/json`
- **ë¬¸ì„œ**: http://localhost:5601/docs (FastAPI ìë™ ìƒì„±)

## ğŸ”— ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡

### ì±„íŒ… API
- **POST /api/v1/chat** - AIì™€ì˜ ì±„íŒ… ê¸°ëŠ¥

### ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API
- **GET /api/v1/system/info** - ì „ì²´ ì‹œìŠ¤í…œ ì •ë³´
- **GET /api/v1/system/status** - í—¬ìŠ¤ì²´í¬
- **GET /api/v1/system/cpu** - CPU ì •ë³´
- **GET /api/v1/system/memory** - ë©”ëª¨ë¦¬ ì •ë³´
- **GET /api/v1/system/disk** - ë””ìŠ¤í¬ ì •ë³´

### ê¸°ë³¸ API
- **GET /api/v1/** - ì„œë²„ ìƒíƒœ í™•ì¸

---

## ğŸ¤– ì±„íŒ… API

### POST /api/v1/chat

AIì™€ì˜ ì±„íŒ…ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.

#### Request Body

```json
{
  "session_id": "string",
  "system_message": "string",
  "user_message": "string",
  "role": "string",
  "instructions": "string",
  "conversation_history": ["string"],
  "memory_context": ["string"],
  "max_tokens": 1000,
  "temperature": 0.7,
  "model": "gpt-4o-mini",
  "openai_api_key": "string",
  "free_mode": false
}
```

#### í•„ë“œ ìƒì„¸ ì„¤ëª…

| í•„ë“œ | íƒ€ì… | í•„ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|------|------|--------|------|
| `session_id` | string | âŒ | "" | ì„¸ì…˜ ID (ë¡œê¹… ë° ì¶”ì ìš©) |
| `system_message` | string | âŒ | "" | ì¶”ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ |
| `user_message` | string | âŒ | "" | ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ |
| `role` | string | âŒ | "" | AI ì—­í•  ì„¤ì • (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨) |
| `instructions` | string | âŒ | "" | ì¶”ê°€ ì§€ì‹œì‚¬í•­ |
| `conversation_history` | string[] | âŒ | [] | ìµœê·¼ ëŒ€í™” ë‚´ì—­ |
| `memory_context` | string[] | âŒ | [] | ì¥ê¸° ê¸°ì–µ Context |
| `max_tokens` | int | âŒ | 1000 | ìµœëŒ€ í† í° ìˆ˜ (0ë³´ë‹¤ ì»¤ì•¼ í•¨) |
| `temperature` | float | âŒ | 0.7 | ìƒì„± ì˜¨ë„ (0-2 ì‚¬ì´) |
| `model` | string | âŒ | "gpt-4o-mini" | ì‚¬ìš©í•  OpenAI ëª¨ë¸ |
| `openai_api_key` | string | âŒ | "" | ì‚¬ìš©í•  OpenAI API Key |
| `free_mode` | bool | âŒ | false | Free ëª¨ë“œ (API Key ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ Key ì‚¬ìš©) |

#### conversation_history í•„ë“œ í˜•ì‹

conversation_history ë°°ì—´ì˜ ê° í•­ëª©ì€ `"role:content"` í˜•íƒœì˜ ë¬¸ìì—´ì…ë‹ˆë‹¤.

- `role`: "user" ë˜ëŠ” "assistant"
- `content`: ì‹¤ì œ ëŒ€í™” ë‚´ìš©

**ì˜ˆì‹œ:**
```json
[
  "user:ì•ˆë…•í•˜ì„¸ìš”",
  "assistant:ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
  "user:íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
]
```

#### API Key ì‚¬ìš© ë°©ì‹

1. **ì¼ë°˜ ëª¨ë“œ** (`free_mode: false`):
   - `openai_api_key`ê°€ ì œê³µë˜ì–´ì•¼ í•¨
   - API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ

2. **Free ëª¨ë“œ** (`free_mode: true`):
   - `openai_api_key`ê°€ ì œê³µë˜ë©´ ìš°ì„  ì‚¬ìš©
   - API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ Key ì‚¬ìš©
   - ì‚¬ìš©ì ê´€ë¦¬ ë° ì‚¬ìš©ëŸ‰ ì¶”ì  ê°€ëŠ¥

#### Response

```json
{
  "session_id": "string",
  "response_text": "string",
  "model": "string",
  "input_tokens": 0,
  "output_tokens": 0,
  "total_tokens_used": 0,
  "output_format": "string",
  "created_at": "2024-01-01T00:00:00",
  "temperature": 0.0,
  "instructions": "string",
  "response_time": 0.0,
  "success": true,
  "error_message": null,
  "api_key_source": "user_provided"
}
```

#### Response í•„ë“œ ì„¤ëª…

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `session_id` | string | ì„¸ì…˜ ID |
| `response_text` | string | AI ì‘ë‹µ í…ìŠ¤íŠ¸ |
| `model` | string | ì‚¬ìš©ëœ OpenAI ëª¨ë¸ëª… |
| `input_tokens` | int | ì…ë ¥ í† í° ìˆ˜ |
| `output_tokens` | int | ì¶œë ¥ í† í° ìˆ˜ |
| `total_tokens_used` | int | ì´ í† í° ìˆ˜ |
| `output_format` | string | ì¶œë ¥ í˜•ì‹ |
| `created_at` | string | ì‘ë‹µ ìƒì„± ì‹œê°„ (ISO 8601) |
| `temperature` | float | ì‚¬ìš©ëœ temperature ê°’ |
| `instructions` | string | ì „ë‹¬ëœ ì§€ì‹œì‚¬í•­ |
| `response_time` | float | ì‘ë‹µ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ) |
| `success` | bool | ì„±ê³µ ì—¬ë¶€ |
| `error_message` | string | ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ) |
| `api_key_source` | string | ì‚¬ìš©ëœ API Key ì†ŒìŠ¤ ("user_provided" ë˜ëŠ” "default") |

#### HTTP ìƒíƒœ ì½”ë“œ

- `200 OK`: ì„±ê³µì ì¸ ì‘ë‹µ
- `400 Bad Request`: ìš”ì²­ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜
- `422 Unprocessable Entity`: ìš”ì²­ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜
- `500 Internal Server Error`: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜
- `503 Service Unavailable`: AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜

#### ì‚¬ìš© ì˜ˆì‹œ

**ê¸°ë³¸ ì‚¬ìš©:**
```python
import requests

response = requests.post("http://localhost:5601/api/v1/chat", json={
    "session_id": "session_123",
    "user_message": "íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
    "role": "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
    "max_tokens": 1000,
    "temperature": 0.7
})

result = response.json()
print(result['response_text'])
```

**Free ëª¨ë“œ ì‚¬ìš©:**
```python
response = requests.post("http://localhost:5601/api/v1/chat", json={
    "session_id": "session_123",
    "user_message": "íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
    "openai_api_key": "sk-your-api-key",
    "free_mode": True,
    "conversation_history": [
        "user:ì•ˆë…•í•˜ì„¸ìš”",
        "assistant:ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    ]
})

result = response.json()
print(f"ì‘ë‹µ: {result['response_text']}")
print(f"API Key ì†ŒìŠ¤: {result['api_key_source']}")
```

---

## ğŸ“Š ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API

### GET /api/v1/system/info

ì „ì²´ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

#### Response

```json
{
  "timestamp": "2024-01-01T00:00:00",
  "system": {
    "platform": "Windows",
    "platform_version": "10.0.26100",
    "architecture": "AMD64",
    "processor": "Intel64 Family 6",
    "hostname": "DESKTOP-XXXXX",
    "python_version": "3.11.0"
  },
  "cpu": {
    "usage_percent": 25.5,
    "count": 8,
    "frequency": 2400.0
  },
  "memory": {
    "total": 8589934592,
    "available": 4294967296,
    "used": 4294967296,
    "usage_percent": 50.0
  },
  "disk": {
    "total": 107374182400,
    "used": 53687091200,
    "free": 53687091200,
    "usage_percent": 50.0
  }
}
```

### GET /api/v1/system/status

ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬ë¥¼ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.

#### Response

```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T00:00:00",
  "uptime": 3600.5
}
```

### GET /api/v1/system/cpu

CPU ì •ë³´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

#### Response

```json
{
  "timestamp": "2024-01-01T00:00:00",
  "cpu": {
    "usage_percent": 25.5,
    "count": 8,
    "frequency": 2400.0
  }
}
```

### GET /api/v1/system/memory

ë©”ëª¨ë¦¬ ì •ë³´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

#### Response

```json
{
  "timestamp": "2024-01-01T00:00:00",
  "memory": {
    "total": 8589934592,
    "available": 4294967296,
    "used": 4294967296,
    "usage_percent": 50.0
  }
}
```

### GET /api/v1/system/disk

ë””ìŠ¤í¬ ì •ë³´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

#### Response

```json
{
  "timestamp": "2024-01-01T00:00:00",
  "disk": {
    "total": 107374182400,
    "used": 53687091200,
    "free": 53687091200,
    "usage_percent": 50.0
  }
}
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **[ê°œìš” ë° ì‹œì‘ ê°€ì´ë“œ](./../overview/README.md)**: í”„ë¡œì íŠ¸ ì†Œê°œ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
- **[ë°°í¬ ê°€ì´ë“œ](./../deployment/README.md)**: ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ
- **[ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](./../architecture/README.md)**: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ê°œë°œ ê°€ì´ë“œ
- **[í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](./../testing/README.md)**: í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ ê´€ë¦¬

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ì‘ì„±ì**: LLM Server ê°œë°œíŒ€ 