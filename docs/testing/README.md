# LLM Server í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ ê´€ë¦¬ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” LLM Serverì˜ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±, ì‹¤í–‰, ê·¸ë¦¬ê³  í’ˆì§ˆ ê´€ë¦¬ì— ëŒ€í•œ ìƒì„¸í•œ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê°œìš”

### í…ŒìŠ¤íŠ¸ ì „ëµ

1. **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Tests)** - ê°œë³„ í•¨ìˆ˜/í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
2. **í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests)** - API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
3. **ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (Scenario Tests)** - ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
4. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Tests)** - ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ë„êµ¬

- **pytest**: Python í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- **httpx**: ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ (API í…ŒìŠ¤íŠ¸)
- **pytest-asyncio**: ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì§€ì›
- **pytest-mock**: Mock ê°ì²´ ì§€ì›

---

## ğŸ“ í…ŒìŠ¤íŠ¸ êµ¬ì¡°

### í”„ë¡œì íŠ¸ í…ŒìŠ¤íŠ¸ êµ¬ì¡°

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_unit.py          # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_integration.py   # í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_scenarios.py     # ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ conftest.py          # pytest ì„¤ì •
â””â”€â”€ fixtures/            # í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ mock_data.py     # Mock ë°ì´í„°
    â””â”€â”€ test_utils.py    # í…ŒìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹°
```

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ëª…ëª… ê·œì¹™

- `test_*.py`: í…ŒìŠ¤íŠ¸ íŒŒì¼
- `test_*`: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
- `Test*`: í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤

---

## ğŸ”§ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### 1. ì„œë¹„ìŠ¤ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸

#### ChatService í…ŒìŠ¤íŠ¸

```python
import pytest
from unittest.mock import Mock, AsyncMock
from src.services.chat_service import ChatService
from src.dto.request_dto import ChatRequest
from src.dto.response_dto import ChatResponse

class TestChatService:
    @pytest.fixture
    def mock_openai_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ Mock ê°ì²´"""
        mock_client = Mock()
        mock_client.chat_completion = AsyncMock()
        return mock_client
    
    @pytest.fixture
    def chat_service(self, mock_openai_client):
        """ChatService ì¸ìŠ¤í„´ìŠ¤"""
        return ChatService(mock_openai_client)
    
    @pytest.fixture
    def sample_request(self):
        """ìƒ˜í”Œ ì±„íŒ… ìš”ì²­"""
        return ChatRequest(
            session_id="test_session",
            user_message="ì•ˆë…•í•˜ì„¸ìš”",
            role="ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸",
            max_tokens=100,
            temperature=0.7
        )
    
    @pytest.mark.asyncio
    async def test_process_chat_success(self, chat_service, mock_openai_client, sample_request):
        """ì„±ê³µì ì¸ ì±„íŒ… ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # Given
        expected_response = Mock()
        expected_response.choices = [Mock()]
        expected_response.choices[0].message.content = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        expected_response.usage.total_tokens = 50
        
        mock_openai_client.chat_completion.return_value = expected_response
        
        # When
        result = await chat_service.process_chat(sample_request)
        
        # Then
        assert result.success is True
        assert result.response_text == "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        assert result.total_tokens_used == 50
        assert result.session_id == "test_session"
        mock_openai_client.chat_completion.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_process_chat_openai_error(self, chat_service, mock_openai_client, sample_request):
        """OpenAI API ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # Given
        mock_openai_client.chat_completion.side_effect = Exception("API ì˜¤ë¥˜")
        
        # When & Then
        with pytest.raises(Exception):
            await chat_service.process_chat(sample_request)
    
    def test_build_context_with_history(self, chat_service, sample_request):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ í¬í•¨ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í…ŒìŠ¤íŠ¸"""
        # Given
        sample_request.conversation_history = [
            "user:ì•ˆë…•í•˜ì„¸ìš”",
            "assistant:ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "user:íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        # When
        context = chat_service._build_context(sample_request)
        
        # Then
        assert len(context.messages) == 5  # ì‹œìŠ¤í…œ + íˆìŠ¤í† ë¦¬ 3ê°œ + í˜„ì¬ ë©”ì‹œì§€
        assert context.messages[0]["role"] == "system"
        assert context.messages[1]["role"] == "user"
        assert context.messages[2]["role"] == "assistant"
        assert context.messages[3]["role"] == "user"
        assert context.messages[4]["role"] == "user"
    
    def test_build_context_with_memory(self, chat_service, sample_request):
        """ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í…ŒìŠ¤íŠ¸"""
        # Given
        sample_request.memory_context = [
            "ì‚¬ìš©ìëŠ” ê°œë°œìì…ë‹ˆë‹¤",
            "ì‚¬ìš©ìëŠ” íŒŒì´ì¬ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤"
        ]
        
        # When
        context = chat_service._build_context(sample_request)
        
        # Then
        assert len(context.messages) == 3  # ì‹œìŠ¤í…œ(ë©”ëª¨ë¦¬) + ì‹œìŠ¤í…œ(ì—­í• ) + ì‚¬ìš©ì
        assert "ê¸°ì–µ: ì‚¬ìš©ìëŠ” ê°œë°œìì…ë‹ˆë‹¤" in context.messages[0]["content"]
        assert "ì‚¬ìš©ìëŠ” íŒŒì´ì¬ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤" in context.messages[0]["content"]
```

### 2. DTO í…ŒìŠ¤íŠ¸

#### ChatRequest ê²€ì¦ í…ŒìŠ¤íŠ¸

```python
import pytest
from pydantic import ValidationError
from src.dto.request_dto import ChatRequest

class TestChatRequest:
    def test_valid_request(self):
        """ìœ íš¨í•œ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        request = ChatRequest(
            session_id="test_session",
            user_message="ì•ˆë…•í•˜ì„¸ìš”",
            max_tokens=1000,
            temperature=0.7
        )
        
        assert request.session_id == "test_session"
        assert request.user_message == "ì•ˆë…•í•˜ì„¸ìš”"
        assert request.max_tokens == 1000
        assert request.temperature == 0.7
    
    def test_empty_user_message_validation(self):
        """ë¹ˆ ì‚¬ìš©ì ë©”ì‹œì§€ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValidationError):
            ChatRequest(user_message="")
    
    def test_invalid_max_tokens_validation(self):
        """ì˜ëª»ëœ max_tokens ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValidationError):
            ChatRequest(user_message="í…ŒìŠ¤íŠ¸", max_tokens=-1)
    
    def test_invalid_temperature_validation(self):
        """ì˜ëª»ëœ temperature ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValidationError):
            ChatRequest(user_message="í…ŒìŠ¤íŠ¸", temperature=3.0)
    
    def test_conversation_history_validation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í˜•ì‹ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        # ìœ íš¨í•œ í˜•ì‹
        request = ChatRequest(
            user_message="í…ŒìŠ¤íŠ¸",
            conversation_history=["user:ì•ˆë…•í•˜ì„¸ìš”", "assistant:ì•ˆë…•í•˜ì„¸ìš”!"]
        )
        assert len(request.conversation_history) == 2
        
        # ì˜ëª»ëœ í˜•ì‹
        with pytest.raises(ValidationError):
            ChatRequest(
                user_message="í…ŒìŠ¤íŠ¸",
                conversation_history=["ì˜ëª»ëœí˜•ì‹"]
            )
```

### 3. ì„¤ì • í…ŒìŠ¤íŠ¸

#### Config í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸

```python
import pytest
import os
from src.config.config import Config
from src.exceptions.custom_exceptions import ConfigurationException

class TestConfig:
    def test_config_loading(self):
        """ì„¤ì • ë¡œë”© í…ŒìŠ¤íŠ¸"""
        # Given
        os.environ["OPENAI_API_KEY"] = "test-key"
        os.environ["SERVER_PORT"] = "8080"
        
        # When
        config = Config()
        
        # Then
        assert config.openai_api_key == "test-key"
        assert config.server_port == 8080
    
    def test_config_defaults(self):
        """ê¸°ë³¸ê°’ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        # Given
        if "OPENAI_API_KEY" in os.environ:
            del os.environ["OPENAI_API_KEY"]
        
        # When
        config = Config()
        
        # Then
        assert config.server_host == "0.0.0.0"
        assert config.server_port == 5601
    
    def test_config_validation_failure(self):
        """ì„¤ì • ê²€ì¦ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸"""
        # Given
        if "OPENAI_API_KEY" in os.environ:
            del os.environ["OPENAI_API_KEY"]
        
        # When & Then
        with pytest.raises(ConfigurationException):
            config = Config()
            config.validate()
```

---

## ğŸŒ í†µí•© í…ŒìŠ¤íŠ¸

### 1. API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

#### FastAPI í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

```python
import pytest
from fastapi.testclient import TestClient
from app import app

@pytest.fixture
def client():
    """FastAPI í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸"""
    return TestClient(app)

class TestChatAPI:
    def test_chat_endpoint_success(self, client):
        """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Given
        payload = {
            "session_id": "test_session",
            "user_message": "ì•ˆë…•í•˜ì„¸ìš”",
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        # When
        response = client.post("/api/v1/chat", json=payload)
        
        # Then
        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "response_text" in data
        assert data["session_id"] == "test_session"
    
    def test_chat_endpoint_validation_error(self, client):
        """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ê²€ì¦ ì˜¤ë¥˜ í…ŒìŠ¤íŠ¸"""
        # Given
        payload = {
            "user_message": "",  # ë¹ˆ ë©”ì‹œì§€
            "max_tokens": -1     # ì˜ëª»ëœ í† í° ìˆ˜
        }
        
        # When
        response = client.post("/api/v1/chat", json=payload)
        
        # Then
        assert response.status_code == 422  # Validation Error
    
    def test_chat_endpoint_free_mode(self, client):
        """Free ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        # Given
        payload = {
            "session_id": "test_session",
            "user_message": "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€",
            "openai_api_key": "sk-invalid-key",
            "free_mode": True
        }
        
        # When
        response = client.post("/api/v1/chat", json=payload)
        
        # Then
        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert data["api_key_source"] in ["user_provided", "default"]

class TestSystemAPI:
    def test_system_info_endpoint(self, client):
        """ì‹œìŠ¤í…œ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        # When
        response = client.get("/api/v1/system/info")
        
        # Then
        assert response.status_code == 200
        data = response.json()
        assert "system" in data
        assert "cpu" in data
        assert "memory" in data
        assert "disk" in data
    
    def test_system_status_endpoint(self, client):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        # When
        response = client.get("/api/v1/system/status")
        
        # Then
        assert response.status_code == 200
        data = response.json()
        assert "status" in data
        assert "cpu_usage" in data
        assert "memory_usage" in data
```

### 2. ë¹„ë™ê¸° API í…ŒìŠ¤íŠ¸

```python
import pytest
import httpx
from httpx import AsyncClient

@pytest.mark.asyncio
class TestAsyncChatAPI:
    async def test_async_chat_endpoint(self):
        """ë¹„ë™ê¸° ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # Given
            payload = {
                "session_id": "async_test",
                "user_message": "ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸",
                "max_tokens": 100
            }
            
            # When
            response = await ac.post("/api/v1/chat", json=payload)
            
            # Then
            assert response.status_code == 200
            data = response.json()
            assert data["success"] is True
    
    async def test_concurrent_requests(self):
        """ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # Given
            payloads = [
                {"user_message": f"í…ŒìŠ¤íŠ¸ {i}", "max_tokens": 50}
                for i in range(5)
            ]
            
            # When
            responses = await asyncio.gather(*[
                ac.post("/api/v1/chat", json=payload)
                for payload in payloads
            ])
            
            # Then
            for response in responses:
                assert response.status_code == 200
                data = response.json()
                assert data["success"] is True
```

---

## ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

### 1. ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

```python
import pytest
from src.services.chat_service import ChatService
from src.dto.request_dto import ChatRequest

class TestChatScenarios:
    @pytest.fixture
    def chat_service(self, mock_openai_client):
        return ChatService(mock_openai_client)
    
    @pytest.mark.asyncio
    async def test_conversation_flow(self, chat_service, mock_openai_client):
        """ëŒ€í™” íë¦„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given - ì²« ë²ˆì§¸ ë©”ì‹œì§€
        request1 = ChatRequest(
            session_id="conversation_test",
            user_message="ì•ˆë…•í•˜ì„¸ìš”",
            role="ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸"
        )
        
        # Mock ì²« ë²ˆì§¸ ì‘ë‹µ
        mock_response1 = Mock()
        mock_response1.choices = [Mock()]
        mock_response1.choices[0].message.content = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        mock_response1.usage.total_tokens = 20
        
        mock_openai_client.chat_completion.return_value = mock_response1
        
        # When - ì²« ë²ˆì§¸ ì‘ë‹µ
        response1 = await chat_service.process_chat(request1)
        
        # Then
        assert response1.success is True
        assert "ì•ˆë…•í•˜ì„¸ìš”" in response1.response_text
        
        # Given - ë‘ ë²ˆì§¸ ë©”ì‹œì§€ (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        request2 = ChatRequest(
            session_id="conversation_test",
            user_message="íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            conversation_history=[
                "user:ì•ˆë…•í•˜ì„¸ìš”",
                "assistant:ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            ]
        )
        
        # Mock ë‘ ë²ˆì§¸ ì‘ë‹µ
        mock_response2 = Mock()
        mock_response2.choices = [Mock()]
        mock_response2.choices[0].message.content = "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤..."
        mock_response2.usage.total_tokens = 50
        
        mock_openai_client.chat_completion.return_value = mock_response2
        
        # When - ë‘ ë²ˆì§¸ ì‘ë‹µ
        response2 = await chat_service.process_chat(request2)
        
        # Then
        assert response2.success is True
        assert "íŒŒì´ì¬" in response2.response_text
        assert response2.total_tokens_used == 50
    
    @pytest.mark.asyncio
    async def test_api_key_fallback_scenario(self, chat_service, mock_openai_client):
        """API Key í´ë°± ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given - ì˜ëª»ëœ API Keyë¡œ ìš”ì²­
        request = ChatRequest(
            session_id="fallback_test",
            user_message="í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€",
            openai_api_key="sk-invalid-key",
            free_mode=True
        )
        
        # Mock - ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹¤íŒ¨, ë‘ ë²ˆì§¸ í˜¸ì¶œ ì„±ê³µ
        mock_openai_client.chat_completion.side_effect = [
            Exception("Invalid API Key"),  # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹¤íŒ¨
            Mock(choices=[Mock()], usage=Mock(total_tokens=30))  # ë‘ ë²ˆì§¸ í˜¸ì¶œ ì„±ê³µ
        ]
        
        # When
        response = await chat_service.process_chat(request)
        
        # Then
        assert response.success is True
        assert mock_openai_client.chat_completion.call_count == 2
    
    @pytest.mark.asyncio
    async def test_memory_context_scenario(self, chat_service, mock_openai_client):
        """ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given
        request = ChatRequest(
            session_id="memory_test",
            user_message="ë‚´ ê´€ì‹¬ì‚¬ì— ëŒ€í•´ ë§í•´ë´",
            memory_context=[
                "ì‚¬ìš©ìëŠ” ê°œë°œìì…ë‹ˆë‹¤",
                "ì‚¬ìš©ìëŠ” AIì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤",
                "ì‚¬ìš©ìëŠ” íŒŒì´ì¬ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤"
            ]
        )
        
        # Mock ì‘ë‹µ
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = "ê°œë°œìì´ì‹œêµ°ìš”! AIì™€ íŒŒì´ì¬ì— ê´€ì‹¬ì´ ë§ìœ¼ì‹œë„¤ìš”."
        mock_response.usage.total_tokens = 40
        
        mock_openai_client.chat_completion.return_value = mock_response
        
        # When
        response = await chat_service.process_chat(request)
        
        # Then
        assert response.success is True
        assert "ê°œë°œì" in response.response_text
```

### 2. ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

```python
class TestErrorScenarios:
    @pytest.mark.asyncio
    async def test_network_timeout_scenario(self, chat_service, mock_openai_client):
        """ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given
        request = ChatRequest(user_message="í…ŒìŠ¤íŠ¸")
        mock_openai_client.chat_completion.side_effect = TimeoutError("Network timeout")
        
        # When & Then
        with pytest.raises(Exception):
            await chat_service.process_chat(request)
    
    @pytest.mark.asyncio
    async def test_rate_limit_scenario(self, chat_service, mock_openai_client):
        """Rate Limit ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given
        request = ChatRequest(user_message="í…ŒìŠ¤íŠ¸")
        mock_openai_client.chat_completion.side_effect = Exception("Rate limit exceeded")
        
        # When & Then
        with pytest.raises(Exception):
            await chat_service.process_chat(request)
    
    @pytest.mark.asyncio
    async def test_invalid_response_scenario(self, chat_service, mock_openai_client):
        """ì˜ëª»ëœ ì‘ë‹µ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given
        request = ChatRequest(user_message="í…ŒìŠ¤íŠ¸")
        mock_response = Mock()
        mock_response.choices = []  # ë¹ˆ ì‘ë‹µ
        mock_openai_client.chat_completion.return_value = mock_response
        
        # When & Then
        with pytest.raises(Exception):
            await chat_service.process_chat(request)
```

---

## âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### 1. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
import asyncio
import time
import pytest
from httpx import AsyncClient

class TestPerformance:
    @pytest.mark.asyncio
    async def test_concurrent_load(self):
        """ë™ì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # Given
            num_requests = 10
            payload = {
                "user_message": "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                "max_tokens": 50
            }
            
            # When
            start_time = time.time()
            responses = await asyncio.gather(*[
                ac.post("/api/v1/chat", json=payload)
                for _ in range(num_requests)
            ])
            end_time = time.time()
            
            # Then
            total_time = end_time - start_time
            avg_time = total_time / num_requests
            
            assert all(response.status_code == 200 for response in responses)
            assert avg_time < 5.0  # í‰ê·  ì‘ë‹µ ì‹œê°„ 5ì´ˆ ì´í•˜
            print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    
    @pytest.mark.asyncio
    async def test_memory_usage(self, client):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        import psutil
        import os
        
        # Given
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # When
        for _ in range(100):
            response = client.post("/api/v1/chat", json={
                "user_message": "ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸",
                "max_tokens": 30
            })
            assert response.status_code == 200
        
        # Then
        final_memory = process.memory_info().rss
        memory_increase = (final_memory - initial_memory) / 1024 / 1024  # MB
        
        assert memory_increase < 100  # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ 100MB ì´í•˜
        print(f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.2f}MB")
```

### 2. ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸

```python
class TestResponseTime:
    def test_chat_response_time(self, client):
        """ì±„íŒ… ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        import time
        
        # Given
        payload = {
            "user_message": "ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸",
            "max_tokens": 100
        }
        
        # When
        start_time = time.time()
        response = client.post("/api/v1/chat", json=payload)
        end_time = time.time()
        
        # Then
        response_time = end_time - start_time
        assert response.status_code == 200
        assert response_time < 10.0  # ì‘ë‹µ ì‹œê°„ 10ì´ˆ ì´í•˜
        print(f"ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
    
    def test_system_info_response_time(self, client):
        """ì‹œìŠ¤í…œ ì •ë³´ ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        import time
        
        # When
        start_time = time.time()
        response = client.get("/api/v1/system/info")
        end_time = time.time()
        
        # Then
        response_time = end_time - start_time
        assert response.status_code == 200
        assert response_time < 1.0  # ì‘ë‹µ ì‹œê°„ 1ì´ˆ ì´í•˜
        print(f"ì‹œìŠ¤í…œ ì •ë³´ ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
```

---

## ğŸ› ï¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### 1. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰
pytest tests/test_unit.py

# íŠ¹ì • í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‹¤í–‰
pytest tests/test_unit.py::TestChatService::test_process_chat_success

# ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest -m asyncio

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest -m "performance"

# ì»¤ë²„ë¦¬ì§€ì™€ í•¨ê»˜ ì‹¤í–‰
pytest --cov=src --cov-report=html

# ìƒì„¸ ì¶œë ¥ê³¼ í•¨ê»˜ ì‹¤í–‰
pytest -v

# ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë§Œ ì¬ì‹¤í–‰
pytest --lf
```

### 2. í…ŒìŠ¤íŠ¸ ì„¤ì •

#### `pytest.ini` ì„¤ì •
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
markers =
    unit: Unit tests
    integration: Integration tests
    scenario: Scenario tests
    performance: Performance tests
    asyncio: Async tests
```

#### `conftest.py` ì„¤ì •
```python
import pytest
import asyncio
from unittest.mock import Mock
from src.services.chat_service import ChatService
from src.external.openai_client import OpenAIClient

@pytest.fixture(scope="session")
def event_loop():
    """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def mock_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ Mock"""
    mock_client = Mock(spec=OpenAIClient)
    mock_client.chat_completion = asyncio.coroutine(Mock())
    return mock_client

@pytest.fixture
def chat_service(mock_openai_client):
    """ChatService ì¸ìŠ¤í„´ìŠ¤"""
    return ChatService(mock_openai_client)
```

### 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬

#### `tests/fixtures/mock_data.py`
```python
"""í…ŒìŠ¤íŠ¸ìš© Mock ë°ì´í„°"""

SAMPLE_CHAT_REQUEST = {
    "session_id": "test_session",
    "user_message": "ì•ˆë…•í•˜ì„¸ìš”",
    "role": "ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    "max_tokens": 100,
    "temperature": 0.7
}

SAMPLE_CHAT_RESPONSE = {
    "session_id": "test_session",
    "response_text": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "model": "gpt-4o-mini",
    "input_tokens": 20,
    "output_tokens": 30,
    "total_tokens_used": 50,
    "response_time": 1.5,
    "success": True,
    "error_message": None,
    "api_key_source": "default"
}

SAMPLE_SYSTEM_INFO = {
    "timestamp": "2024-01-01T00:00:00",
    "system": {
        "platform": "Windows",
        "platform_version": "10.0.26100",
        "architecture": "AMD64",
        "hostname": "test-host",
        "python_version": "3.11.0"
    },
    "cpu": {
        "usage_percent": 15.2,
        "count": 8,
        "frequency_mhz": 2400.0
    },
    "memory": {
        "total_gb": 16.0,
        "available_gb": 8.0,
        "usage_percent": 50.0
    }
}
```

---

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬

### 1. ì½”ë“œ ì»¤ë²„ë¦¬ì§€

#### ì»¤ë²„ë¦¬ì§€ ì„¤ì •
```bash
# ì»¤ë²„ë¦¬ì§€ ì„¤ì¹˜
pip install pytest-cov

# ì»¤ë²„ë¦¬ì§€ ì‹¤í–‰
pytest --cov=src --cov-report=html --cov-report=term-missing

# ì»¤ë²„ë¦¬ì§€ ì„ê³„ê°’ ì„¤ì •
pytest --cov=src --cov-fail-under=80
```

#### `.coveragerc` ì„¤ì •
```ini
[run]
source = src
omit = 
    */tests/*
    */__pycache__/*
    */venv/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod
```

### 2. ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

#### flake8 ì„¤ì •
```bash
# flake8 ì„¤ì¹˜
pip install flake8

# ì½”ë“œ ê²€ì‚¬
flake8 src/ tests/

# ì„¤ì • íŒŒì¼ (.flake8)
[flake8]
max-line-length = 100
exclude = .git,__pycache__,build,dist,*.egg-info
ignore = E203, W503
```

#### black ì„¤ì •
```bash
# black ì„¤ì¹˜
pip install black

# ì½”ë“œ í¬ë§·íŒ…
black src/ tests/

# ì„¤ì • íŒŒì¼ (pyproject.toml)
[tool.black]
line-length = 100
target-version = ['py38']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''
```

### 3. íƒ€ì… ê²€ì‚¬

#### mypy ì„¤ì •
```bash
# mypy ì„¤ì¹˜
pip install mypy

# íƒ€ì… ê²€ì‚¬
mypy src/

# ì„¤ì • íŒŒì¼ (mypy.ini)
[mypy]
python_version = 3.8
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
warn_unreachable = True
strict_equality = True

[mypy-pydantic.*]
ignore_missing_imports = True
```

### 4. ë³´ì•ˆ ê²€ì‚¬

#### bandit ì„¤ì •
```bash
# bandit ì„¤ì¹˜
pip install bandit

# ë³´ì•ˆ ê²€ì‚¬
bandit -r src/

# ì„¤ì • íŒŒì¼ (.bandit)
exclude_dirs = ['tests']
skips = ['B101']
```

---

## ğŸ”„ CI/CD í†µí•©

### 1. GitHub Actions ì„¤ì •

#### `.github/workflows/test.yml`
```yaml
name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirement.txt
        pip install pytest pytest-cov flake8 black mypy bandit
    
    - name: Run tests
      run: |
        pytest --cov=src --cov-report=xml
    
    - name: Run linting
      run: |
        flake8 src/ tests/
        black --check src/ tests/
    
    - name: Run type checking
      run: |
        mypy src/
    
    - name: Run security checks
      run: |
        bandit -r src/
    
    - name: Upload coverage
      uses: codecov/codecov-action@v1
      with:
        file: ./coverage.xml
```

### 2. í…ŒìŠ¤íŠ¸ ìë™í™”

#### `run_tests.py` ìŠ¤í¬ë¦½íŠ¸
```python
#!/usr/bin/env python3
"""í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"""

import subprocess
import sys
import os

def run_command(command, description):
    """ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"\nğŸ”„ {description}...")
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"âœ… {description} ì™„ë£Œ")
        if result.stdout:
            print(result.stdout)
    else:
        print(f"âŒ {description} ì‹¤íŒ¨")
        print(result.stderr)
        return False
    
    return True

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª LLM Server í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    
    # í…ŒìŠ¤íŠ¸ ìˆœì„œ
    tests = [
        ("pytest --cov=src --cov-report=term-missing", "ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸"),
        ("flake8 src/ tests/", "ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€ì‚¬"),
        ("black --check src/ tests/", "ì½”ë“œ í¬ë§·íŒ… ê²€ì‚¬"),
        ("mypy src/", "íƒ€ì… ê²€ì‚¬"),
        ("bandit -r src/", "ë³´ì•ˆ ê²€ì‚¬")
    ]
    
    all_passed = True
    
    for command, description in tests:
        if not run_command(command, description):
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        sys.exit(0)
    else:
        print("\nğŸ’¥ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## ğŸ“ˆ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­

### 1. í…ŒìŠ¤íŠ¸ ì§€í‘œ

- **ì»¤ë²„ë¦¬ì§€**: 80% ì´ìƒ ëª©í‘œ
- **ì‘ë‹µ ì‹œê°„**: í‰ê·  5ì´ˆ ì´í•˜
- **ë™ì‹œ ìš”ì²­**: 10ê°œ ì´ìƒ ì²˜ë¦¬ ê°€ëŠ¥
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 100MB ì´í•˜ ì¦ê°€

### 2. í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

```bash
# HTML ë¦¬í¬íŠ¸ ìƒì„±
pytest --cov=src --cov-report=html

# XML ë¦¬í¬íŠ¸ ìƒì„± (CI/CDìš©)
pytest --cov=src --cov-report=xml

# í„°ë¯¸ë„ ë¦¬í¬íŠ¸
pytest --cov=src --cov-report=term-missing
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **[ê°œìš” ë° ì‹œì‘ ê°€ì´ë“œ](./../overview/README.md)**: í”„ë¡œì íŠ¸ ì†Œê°œ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
- **[API ë¬¸ì„œ](./../api/README.md)**: API ëª…ì„¸ ë° ì‚¬ìš©ë²•
- **[ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](./../architecture/README.md)**: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ê°œë°œ ê°€ì´ë“œ
- **[ë°°í¬ ê°€ì´ë“œ](./../deployment/README.md)**: ìš´ì˜ í™˜ê²½ ë°°í¬ ë°©ë²•

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ì‘ì„±ì**: LLM Server ê°œë°œíŒ€ 