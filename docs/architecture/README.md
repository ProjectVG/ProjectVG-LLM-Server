# LLM Server ì•„í‚¤í…ì²˜ ë° ê°œë°œ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” LLM Serverì˜ ì•„í‚¤í…ì²˜ ì„¤ê³„, ê°œë°œ ê°€ì´ë“œë¼ì¸, ê·¸ë¦¬ê³  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### ì „ì²´ êµ¬ì¡°

```
LLM Server
â”œâ”€â”€ ğŸŒ API Layer (FastAPI)
â”‚   â”œâ”€â”€ routes.py          # ì±„íŒ… API ë¼ìš°í„°
â”‚   â”œâ”€â”€ system_routes.py   # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API
â”‚   â””â”€â”€ exception_handlers.py # ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬
â”œâ”€â”€ ğŸ¢ Service Layer
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ chat_service.py # ì±„íŒ… ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”œâ”€â”€ ğŸ”— External Layer
â”‚   â””â”€â”€ external/
â”‚       â””â”€â”€ openai_client.py # OpenAI API ì—°ë™
â”œâ”€â”€ ğŸ“¦ DTO Layer
â”‚   â””â”€â”€ dto/
â”‚       â”œâ”€â”€ request_dto.py  # ìš”ì²­ ë°ì´í„° ëª¨ë¸
â”‚       â””â”€â”€ response_dto.py # ì‘ë‹µ ë°ì´í„° ëª¨ë¸
â”œâ”€â”€ âš™ï¸ Config Layer
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config.py       # í™˜ê²½ ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ ğŸ› ï¸ Utils Layer
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py       # ë¡œê¹… ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ system_info.py  # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
â””â”€â”€ âš ï¸ Exception Layer
    â””â”€â”€ exceptions/
        â””â”€â”€ custom_exceptions.py # ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤
```

### ì•„í‚¤í…ì²˜ ì›ì¹™

1. **ê³„ì¸µ ë¶„ë¦¬ (Layered Architecture)**
   - API Layer: HTTP ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬
   - Service Layer: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
   - External Layer: ì™¸ë¶€ API ì—°ë™
   - DTO Layer: ë°ì´í„° ì „ì†¡ ê°ì²´

2. **ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (Separation of Concerns)**
   - ê° ëª¨ë“ˆì€ ë‹¨ì¼ ì±…ì„ì„ ê°€ì§
   - ì˜ì¡´ì„± ìµœì†Œí™”
   - í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´

3. **ì˜ì¡´ì„± ì£¼ì… (Dependency Injection)**
   - ëŠìŠ¨í•œ ê²°í•© (Loose Coupling)
   - í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°
   - í™•ì¥ì„± í–¥ìƒ

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„¸

### 1. API Layer (`src/api/`)

#### `routes.py`
```python
# ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    # ìš”ì²­ ê²€ì¦ ë° ì„œë¹„ìŠ¤ í˜¸ì¶œ
    return await chat_service.process_chat(request)
```

#### `system_routes.py`
```python
# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API ì—”ë“œí¬ì¸íŠ¸
@router.get("/info")
async def get_system_info():
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ë° ë°˜í™˜
    return system_info_collector.get_full_info()
```

#### `exception_handlers.py`
```python
# ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
@app.exception_handler(ValidationException)
async def validation_exception_handler(request, exc):
    # ê²€ì¦ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
    return create_error_response(exc)
```

### 2. Service Layer (`src/services/`)

#### `chat_service.py`
```python
class ChatService:
    def __init__(self, openai_client: OpenAIClient):
        self.openai_client = openai_client
    
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        # 1. ìš”ì²­ ê²€ì¦
        self._validate_request(request)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(request)
        
        # 3. OpenAI API í˜¸ì¶œ
        response = await self.openai_client.chat_completion(context)
        
        # 4. ì‘ë‹µ ìƒì„±
        return self._create_response(response, request)
```

### 3. External Layer (`src/external/`)

#### `openai_client.py`
```python
class OpenAIClient:
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
    
    async def chat_completion(self, context: ChatContext) -> OpenAIResponse:
        # OpenAI API í˜¸ì¶œ ë¡œì§
        response = await self.client.chat.completions.create(
            model=context.model,
            messages=context.messages,
            max_tokens=context.max_tokens,
            temperature=context.temperature
        )
        return response
```

### 4. DTO Layer (`src/dto/`)

#### `request_dto.py`
```python
class ChatRequest(BaseModel):
    session_id: str = ""
    user_message: str
    role: str = ""
    max_tokens: int = 1000
    temperature: float = 0.7
    # ... ê¸°íƒ€ í•„ë“œë“¤
    
    @validator('user_message')
    def validate_user_message(cls, v):
        if not v.strip():
            raise ValueError("ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return v
```

#### `response_dto.py`
```python
class ChatResponse(BaseModel):
    session_id: str
    response_text: str
    total_tokens_used: int
    response_time: float
    success: bool
    error_message: Optional[str] = None
    # ... ê¸°íƒ€ í•„ë“œë“¤
```

### 5. Config Layer (`src/config/`)

#### `config.py`
```python
class Config:
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")
        self.server_host = os.getenv("SERVER_HOST", "0.0.0.0")
        self.server_port = int(os.getenv("SERVER_PORT", "5601"))
        # ... ê¸°íƒ€ ì„¤ì •ë“¤
    
    def validate(self):
        if not self.openai_api_key:
            raise ConfigurationException("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
```

---

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„

### 1. ì±„íŒ… ì„œë¹„ìŠ¤ (ChatService)

#### ì±…ì„
- ì±„íŒ… ìš”ì²­ ì²˜ë¦¬
- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
- OpenAI API í˜¸ì¶œ
- ì‘ë‹µ ìƒì„±

#### ì£¼ìš” ë©”ì„œë“œ
```python
class ChatService:
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        """ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
    def _build_context(self, request: ChatRequest) -> ChatContext:
        """ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ OpenAI API í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        
    def _create_response(self, openai_response: OpenAIResponse, 
                        request: ChatRequest) -> ChatResponse:
        """OpenAI ì‘ë‹µì„ ChatResponseë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
```

#### ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë¡œì§
```python
def _build_context(self, request: ChatRequest) -> ChatContext:
    messages = []
    
    # 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
    if request.role:
        messages.append({"role": "system", "content": request.role})
    
    # 2. ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if request.memory_context:
        memory_text = "\n".join(request.memory_context)
        messages.append({"role": "system", "content": f"ê¸°ì–µ: {memory_text}"})
    
    # 3. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
    for history in request.conversation_history:
        role, content = history.split(":", 1)
        messages.append({"role": role, "content": content})
    
    # 4. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": request.user_message})
    
    return ChatContext(
        messages=messages,
        model=request.model,
        max_tokens=request.max_tokens,
        temperature=request.temperature
    )
```

### 2. OpenAI í´ë¼ì´ì–¸íŠ¸ (OpenAIClient)

#### ì±…ì„
- OpenAI API ì—°ë™
- API Key ê´€ë¦¬
- ì‘ë‹µ ì²˜ë¦¬

#### ì£¼ìš” ë©”ì„œë“œ
```python
class OpenAIClient:
    async def chat_completion(self, context: ChatContext) -> OpenAIResponse:
        """OpenAI Chat Completion APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        
    def _select_api_key(self, user_key: str = None, free_mode: bool = False) -> str:
        """ì‚¬ìš©í•  API Keyë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        
    def _validate_api_key(self, api_key: str) -> bool:
        """API Keyì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
```

#### API Key ì„ íƒ ë¡œì§
```python
def _select_api_key(self, user_key: str = None, free_mode: bool = False) -> str:
    # 1. ì‚¬ìš©ì Keyê°€ ì œê³µëœ ê²½ìš°
    if user_key:
        if free_mode:
            # Free ëª¨ë“œ: ìœ íš¨ì„± ê²€ì¦ í›„ ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ Key
            if self._validate_api_key(user_key):
                return user_key, "user_provided"
            else:
                return self.default_key, "default"
        else:
            # ì¼ë°˜ ëª¨ë“œ: ì‚¬ìš©ì Keyë§Œ ì‚¬ìš©
            return user_key, "user_provided"
    
    # 2. ê¸°ë³¸ Key ì‚¬ìš©
    return self.default_key, "default"
```

### 3. ì„¤ì • ê´€ë¦¬ (Config)

#### ì±…ì„
- í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
- ê¸°ë³¸ê°’ ì„¤ì •
- ì„¤ì • ê²€ì¦

#### ì£¼ìš” ê¸°ëŠ¥
```python
class Config:
    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
        self._load_from_env()
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        self._set_defaults()
        
        # ì„¤ì • ê²€ì¦
        self.validate()
    
    def _load_from_env(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        
    def _set_defaults(self):
        """ê¸°ë³¸ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        
    def validate(self):
        """ì„¤ì •ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
```

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œë¼ì¸

### 1. ì½”ë“œ ìŠ¤íƒ€ì¼

#### Python ì½”ë”© ì»¨ë²¤ì…˜
```python
# íŒŒì¼ëª…: snake_case
# í´ë˜ìŠ¤ëª…: PascalCase
# í•¨ìˆ˜/ë³€ìˆ˜ëª…: snake_case
# ìƒìˆ˜: UPPER_SNAKE_CASE

class ChatService:
    def __init__(self, openai_client: OpenAIClient):
        self.openai_client = openai_client
        self.logger = logging.getLogger(__name__)
    
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        """ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
            return await self._handle_chat_request(request)
        except Exception as e:
            self.logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
```

#### ì£¼ì„ ì‘ì„± ê°€ì´ë“œ
```python
def _build_context(self, request: ChatRequest) -> ChatContext:
    """
    ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ OpenAI API í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    
    Args:
        request (ChatRequest): ì±„íŒ… ìš”ì²­ ê°ì²´
        
    Returns:
        ChatContext: OpenAI API í˜¸ì¶œì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸
        
    Raises:
        ValidationException: ìš”ì²­ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
```

### 2. ì˜ˆì™¸ ì²˜ë¦¬

#### ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì •ì˜
```python
class ValidationException(Exception):
    """ìš”ì²­ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    pass

class ConfigurationException(Exception):
    """ì„¤ì • ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    pass

class ChatServiceException(Exception):
    """ì±„íŒ… ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    pass

class OpenAIClientException(Exception):
    """OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    pass
```

#### ì˜ˆì™¸ ì²˜ë¦¬ íŒ¨í„´
```python
async def process_chat(self, request: ChatRequest) -> ChatResponse:
    try:
        # 1. ìš”ì²­ ê²€ì¦
        self._validate_request(request)
        
        # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰
        result = await self._execute_chat_logic(request)
        
        return result
        
    except ValidationException as e:
        # ê²€ì¦ ì˜¤ë¥˜ ì²˜ë¦¬
        self.logger.warning(f"ìš”ì²­ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise
        
    except OpenAIClientException as e:
        # OpenAI API ì˜¤ë¥˜ ì²˜ë¦¬
        self.logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
        raise ChatServiceException(f"AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
        
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì²˜ë¦¬
        self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise ChatServiceException(f"ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
```

### 3. ë¡œê¹…

#### ë¡œê¹… ì„¤ì •
```python
import logging

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

#### ë¡œê¹… íŒ¨í„´
```python
class ChatService:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        self.logger.info(f"ì±„íŒ… ìš”ì²­ ì‹œì‘: session_id={request.session_id}")
        
        try:
            result = await self._process_request(request)
            self.logger.info(f"ì±„íŒ… ìš”ì²­ ì™„ë£Œ: session_id={request.session_id}, "
                           f"tokens={result.total_tokens_used}")
            return result
            
        except Exception as e:
            self.logger.error(f"ì±„íŒ… ìš”ì²­ ì‹¤íŒ¨: session_id={request.session_id}, "
                            f"error={str(e)}")
            raise
```

### 4. í…ŒìŠ¤íŠ¸ ì‘ì„±

#### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ íŒ¨í„´
```python
import pytest
from unittest.mock import Mock, AsyncMock

class TestChatService:
    @pytest.fixture
    def mock_openai_client(self):
        return Mock(spec=OpenAIClient)
    
    @pytest.fixture
    def chat_service(self, mock_openai_client):
        return ChatService(mock_openai_client)
    
    async def test_process_chat_success(self, chat_service, mock_openai_client):
        # Given
        request = ChatRequest(user_message="í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€")
        expected_response = ChatResponse(...)
        mock_openai_client.chat_completion.return_value = expected_response
        
        # When
        result = await chat_service.process_chat(request)
        
        # Then
        assert result.success is True
        assert result.response_text == expected_response.content
        mock_openai_client.chat_completion.assert_called_once()
```

#### í†µí•© í…ŒìŠ¤íŠ¸ íŒ¨í„´
```python
class TestChatAPI:
    async def test_chat_endpoint_success(self, client):
        # Given
        payload = {
            "user_message": "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€",
            "session_id": "test_session"
        }
        
        # When
        response = await client.post("/api/v1/chat", json=payload)
        
        # Then
        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "response_text" in data
```

---

## ğŸ”„ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### 1. ê¸°ëŠ¥ ê°œë°œ í”„ë¡œì„¸ìŠ¤

1. **ìš”êµ¬ì‚¬í•­ ë¶„ì„**
   - ê¸°ëŠ¥ ëª…ì„¸ ì‘ì„±
   - API ì„¤ê³„
   - ë°ì´í„° ëª¨ë¸ ì •ì˜

2. **ì½”ë“œ êµ¬í˜„**
   - DTO í´ë˜ìŠ¤ ì‘ì„±
   - ì„œë¹„ìŠ¤ ë¡œì§ êµ¬í˜„
   - API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

3. **í…ŒìŠ¤íŠ¸ ì‘ì„±**
   - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
   - í†µí•© í…ŒìŠ¤íŠ¸
   - API í…ŒìŠ¤íŠ¸

4. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**
   - API ë¬¸ì„œ ì—…ë°ì´íŠ¸
   - README ì—…ë°ì´íŠ¸
   - ì½”ë“œ ì£¼ì„ ì¶”ê°€

### 2. ì½”ë“œ ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì½”ë“œ ìŠ¤íƒ€ì¼ ì¤€ìˆ˜
- [ ] ì˜ˆì™¸ ì²˜ë¦¬ ì ì ˆì„±
- [ ] ë¡œê¹… ì¶”ê°€
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 3. ë°°í¬ í”„ë¡œì„¸ìŠ¤

1. **ê°œë°œ í™˜ê²½ í…ŒìŠ¤íŠ¸**
   ```bash
   python run_tests.py
   ```

2. **ë¡œì»¬ ì„œë²„ í…ŒìŠ¤íŠ¸**
   ```bash
   python app.py
   ```

3. **Docker ë¹Œë“œ í…ŒìŠ¤íŠ¸**
   ```bash
   docker-compose up --build
   ```

4. **í”„ë¡œë•ì…˜ ë°°í¬**
   ```bash
   # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
   export OPENAI_API_KEY="your-api-key"
   
   # Docker ë°°í¬
   docker-compose -f docker-compose.prod.yml up -d
   ```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **[ê°œìš” ë° ì‹œì‘ ê°€ì´ë“œ](./../overview/README.md)**: í”„ë¡œì íŠ¸ ì†Œê°œ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
- **[API ë¬¸ì„œ](./../api/README.md)**: API ëª…ì„¸ ë° ì‚¬ìš©ë²•
- **[ë°°í¬ ê°€ì´ë“œ](./../deployment/README.md)**: ìš´ì˜ í™˜ê²½ ë°°í¬ ë°©ë²•
- **[í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](./../testing/README.md)**: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ì‘ì„±ì**: LLM Server ê°œë°œíŒ€ 