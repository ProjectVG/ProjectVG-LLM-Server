# LLM Server ì•„í‚¤í…ì²˜ ë° ê°œë°œ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” LLM Serverì˜ ì•„í‚¤í…ì²˜ ì„¤ê³„, ê°œë°œ ê°€ì´ë“œë¼ì¸, ê·¸ë¦¬ê³  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### ì „ì²´ êµ¬ì¡°

```
LLM Server
â”œâ”€â”€ ğŸŒ API Layer (FastAPI)
â”‚   â”œâ”€â”€ routes.py          # ì±„íŒ… API ë¼ìš°í„°
â”‚   â”œâ”€â”€ system_routes.py   # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API
â”‚   â””â”€â”€ exception_handlers.py # ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬
â”œâ”€â”€ ğŸ¢ Service Layer
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ chat_service.py # ì±„íŒ… ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”œâ”€â”€ ğŸ”— External Layer
â”‚   â””â”€â”€ external/
â”‚       â””â”€â”€ openai_client.py # OpenAI API ì—°ë™
â”œâ”€â”€ ğŸ“¦ DTO Layer
â”‚   â””â”€â”€ dto/
â”‚       â”œâ”€â”€ request_dto.py  # ìš”ì²­ ë°ì´í„° ëª¨ë¸
â”‚       â””â”€â”€ response_dto.py # ì‘ë‹µ ë°ì´í„° ëª¨ë¸
â”œâ”€â”€ âš™ï¸ Config Layer
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config.py       # í™˜ê²½ ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ ğŸ› ï¸ Utils Layer
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py       # ë¡œê¹… ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ system_info.py  # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
â””â”€â”€ âš ï¸ Exception Layer
    â””â”€â”€ exceptions/
        â””â”€â”€ chat_exceptions.py # ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤
```

### ì•„í‚¤í…ì²˜ ì›ì¹™

1. **ê³„ì¸µ ë¶„ë¦¬ (Layered Architecture)**
   - API Layer: HTTP ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬
   - Service Layer: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
   - External Layer: ì™¸ë¶€ API ì—°ë™
   - DTO Layer: ë°ì´í„° ì „ì†¡ ê°ì²´

2. **ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (Separation of Concerns)**
   - ê° ëª¨ë“ˆì€ ë‹¨ì¼ ì±…ì„ì„ ê°€ì§
   - ì˜ì¡´ì„± ìµœì†Œí™”
   - í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´

3. **ì˜ì¡´ì„± ì£¼ì… (Dependency Injection)**
   - ëŠìŠ¨í•œ ê²°í•© (Loose Coupling)
   - í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°
   - í™•ì¥ì„± í–¥ìƒ

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„¸

### 1. API Layer (`src/api/`)

#### `routes.py`
```python
# ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
@router.post("/chat", response_model=ChatResponse)
async def chat_with_ai(request: ChatRequest):
    # ìš”ì²­ ê²€ì¦ ë° ì„œë¹„ìŠ¤ í˜¸ì¶œ
    return chat_service.process_chat_request(request)
```

#### `system_routes.py`
```python
# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ API ì—”ë“œí¬ì¸íŠ¸
@router.get("/info")
async def get_system_info():
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ë° ë°˜í™˜
    return SystemInfoCollector.get_system_info()
```

#### `exception_handlers.py`
```python
# ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
@app.exception_handler(ValidationException)
async def validation_exception_handler(request, exc):
    # ê²€ì¦ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
    return create_error_response(exc)
```

### 2. Service Layer (`src/services/`)

#### `chat_service.py`
```python
class ChatService:
    def __init__(self):
        self.openai_client = OpenAIClient()
    
    def process_chat_request(self, request: ChatRequest) -> ChatResponse:
        # 1. ìš”ì²­ ê²€ì¦
        self._validate_request(request)
        
        # 2. ë©”ì‹œì§€ êµ¬ì„±
        system_message = self._create_system_message(request)
        conversation_history = self._format_conversation_history(request.conversation_history or [])
        user_message = self._create_user_message(request.user_message)
        
        # 3. OpenAI API í˜¸ì¶œ
        openai_response, response_time, api_key_source = self.openai_client.generate_response(
            messages=[system_message] + conversation_history + [user_message],
            api_key=request.openai_api_key,
            free_mode=request.free_mode,
            model=request.model,
            instructions=request.instructions,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        # 4. ì‘ë‹µ ìƒì„±
        return ChatResponse.from_openai_response(
            openai_response=openai_response,
            session_id=request.session_id,
            response_time=response_time,
            api_key_source=api_key_source
        )
```

### 3. External Layer (`src/external/`)

#### `openai_client.py`
```python
class OpenAIClient:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or self._load_api_key()
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
        else:
            self.client = None
    
    def generate_response(
        self,
        messages: list[dict],
        api_key: str = None,
        free_mode: bool = False,
        model: str = DEFAULT_MODEL,
        instructions: str = "",
        max_tokens: int = DEFAULT_MAX_TOKENS,
        temperature: float = DEFAULT_TEMPERATURE
    ) -> tuple[Response, float, str]:
        # API Key ì„ íƒ ë° ê²€ì¦
        selected_api_key, api_key_source = self._select_api_key(api_key, free_mode)
        
        # OpenAI API í˜¸ì¶œ
        client = OpenAI(api_key=selected_api_key)
        response = client.responses.create(
            model=model,
            input=messages,
            instructions=instructions,
            temperature=temperature,
            max_output_tokens=max_tokens
        )
        
        return response, response_time, api_key_source
```

### 4. DTO Layer (`src/dto/`)

#### `request_dto.py`
```python
class ChatRequest(BaseModel):
    session_id: Optional[str] = ""
    system_message: Optional[str] = ""
    user_message: Optional[str] = ""
    role: Optional[str] = ""
    instructions: Optional[str] = ""
    conversation_history: Optional[List[str]] = []
    memory_context: Optional[List[str]] = []
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.7
    model: Optional[str] = "gpt-4o-mini"
    openai_api_key: Optional[str] = ""
    free_mode: Optional[bool] = False
    
    def get_system_message(self) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ ë°˜í™˜"""
        system_prompt = f"""
{self.system_message}

{self._format_role()}

{self._format_memory()}

{self._format_instructions()}
        """
        return system_prompt.strip()
```

#### `response_dto.py`
```python
@dataclass
class ChatResponse:
    session_id: str
    response_text: str
    model: str
    input_tokens: int
    output_tokens: int
    total_tokens_used: int
    output_format: str
    created_at: datetime
    temperature: Optional[float] = None
    instructions: Optional[str] = None
    response_time: Optional[float] = None
    success: bool = True
    error_message: Optional[str] = None
    api_key_source: Optional[str] = None
    
    @classmethod
    def from_openai_response(cls, openai_response, session_id: str = "", response_time: float = None, api_key_source: str = None):
        """OpenAI Responseì—ì„œ ChatResponse ìƒì„±"""
        return cls(
            session_id=session_id,
            response_text=openai_response.output_text,
            model=openai_response.model,
            input_tokens=openai_response.usage.input_tokens,
            output_tokens=openai_response.usage.output_tokens,
            total_tokens_used=openai_response.usage.total_tokens,
            output_format=openai_response.text.format.type,
            created_at=datetime.fromtimestamp(openai_response.created_at),
            temperature=openai_response.temperature,
            response_time=response_time,
            success=True,
            api_key_source=api_key_source
        )
```

### 5. Config Layer (`src/config/`)

#### `config.py`
```python
class Config:
    """ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    SERVER_PORT = "5601"
    OPENAI_API_KEY = ""
    LOG_LEVEL = "INFO"
    LOG_FILE = "logs/app.log"
    DEFAULT_MODEL = "gpt-4o-mini"
    DEFAULT_TEMPERATURE = "0.7"
    DEFAULT_MAX_TOKENS = "1000"
    DEFAULT_SYSTEM_MESSAGE = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    
    def __init__(self):
        self._load_env_file()
        self._load_from_env()
    
    def get(self, key: str, default: Optional[str] = None) -> str:
        """ì„¤ì • ê°’ì—ì„œ ê°’ì„ ê°€ì ¸ì˜´"""
        return getattr(self, key, default)
    
    def get_int(self, key: str, default: int = 0) -> int:
        """ì„¤ì • ê°’ì—ì„œ ì •ìˆ˜ ê°’ì„ ê°€ì ¸ì˜´"""
        value = self.get(key)
        try:
            return int(value) if value else default
        except (ValueError, TypeError):
            return default
```

---

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„

### 1. ì±„íŒ… ì„œë¹„ìŠ¤ (ChatService)

#### ì±…ì„
- ì±„íŒ… ìš”ì²­ ì²˜ë¦¬
- ë©”ì‹œì§€ êµ¬ì„±
- OpenAI API í˜¸ì¶œ
- ì‘ë‹µ ìƒì„±

#### ì£¼ìš” ë©”ì„œë“œ
```python
class ChatService:
    def process_chat_request(self, request: ChatRequest) -> ChatResponse:
        """ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
    def _validate_request(self, request: ChatRequest) -> None:
        """ìš”ì²­ ë°ì´í„° ê²€ì¦"""
        
    def _create_system_message(self, request: ChatRequest) -> dict:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±"""
        
    def _format_conversation_history(self, history: list[str]) -> list[dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
```

### 2. OpenAI í´ë¼ì´ì–¸íŠ¸ (OpenAIClient)

#### ì±…ì„
- OpenAI API í†µì‹ 
- API Key ê´€ë¦¬
- ì‘ë‹µ ì²˜ë¦¬

#### ì£¼ìš” ë©”ì„œë“œ
```python
class OpenAIClient:
    def generate_response(
        self,
        messages: list[dict],
        api_key: str = None,
        free_mode: bool = False,
        model: str = DEFAULT_MODEL,
        instructions: str = "",
        max_tokens: int = DEFAULT_MAX_TOKENS,
        temperature: float = DEFAULT_TEMPERATURE
    ) -> tuple[Response, float, str]:
        """OpenAI APIì— ë©”ì‹œì§€ ì „ì†¡í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        
    def _select_api_key(self, api_key: str = None, free_mode: bool = False) -> tuple[str, str]:
        """API Key ì„ íƒ ë° ê²€ì¦"""
        
    def _validate_api_key(self, api_key: str) -> bool:
        """API Key ìœ íš¨ì„± ê²€ì¦"""
```

### 3. ì˜ˆì™¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ

#### ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤
```python
class ValidationException(Exception):
    """ìš”ì²­ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    
class ConfigurationException(Exception):
    """ì„¤ì • ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    
class ChatServiceException(Exception):
    """ì±„íŒ… ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    
class OpenAIClientException(Exception):
    """OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
```

#### ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
```python
@app.exception_handler(ValidationException)
async def validation_exception_handler(request, exc):
    """ê²€ì¦ ì˜ˆì™¸ ì²˜ë¦¬"""
    
@app.exception_handler(OpenAIClientException)
async def openai_client_exception_handler(request, exc):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì˜ˆì™¸ ì²˜ë¦¬"""
    
@app.exception_handler(ChatServiceException)
async def chat_service_exception_handler(request, exc):
    """ì±„íŒ… ì„œë¹„ìŠ¤ ì˜ˆì™¸ ì²˜ë¦¬"""
```

---

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### 1. ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ í”Œë¡œìš°

```
1. HTTP ìš”ì²­ ìˆ˜ì‹  (POST /api/v1/chat)
   â†“
2. ChatRequest DTO ìƒì„± ë° ê²€ì¦
   â†“
3. ChatService.process_chat_request() í˜¸ì¶œ
   â†“
4. ìš”ì²­ ë°ì´í„° ê²€ì¦ (_validate_request)
   â†“
5. ë©”ì‹œì§€ êµ¬ì„± (_create_system_message, _format_conversation_history)
   â†“
6. OpenAI API í˜¸ì¶œ (OpenAIClient.generate_response)
   â†“
7. ì‘ë‹µ ìƒì„± (ChatResponse.from_openai_response)
   â†“
8. HTTP ì‘ë‹µ ë°˜í™˜
```

### 2. API Key ì²˜ë¦¬ í”Œë¡œìš°

```
1. ìš”ì²­ì—ì„œ API Key í™•ì¸
   â†“
2. Free ëª¨ë“œ ì—¬ë¶€ í™•ì¸
   â†“
3. API Key ì„ íƒ (_select_api_key)
   â”œâ”€ Free ëª¨ë“œ: ì‚¬ìš©ì Key â†’ ê¸°ë³¸ Key
   â””â”€ ì¼ë°˜ ëª¨ë“œ: ì‚¬ìš©ì Keyë§Œ í—ˆìš©
   â†“
4. API Key ìœ íš¨ì„± ê²€ì¦ (_validate_api_key)
   â†“
5. OpenAI API í˜¸ì¶œ
   â†“
6. API Key ì†ŒìŠ¤ ì •ë³´ ì‘ë‹µì— í¬í•¨
```

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œë¼ì¸

### 1. ì½”ë“œ ìŠ¤íƒ€ì¼

#### Python ì½”ë”© ì»¨ë²¤ì…˜
- **PEP 8** ì¤€ìˆ˜
- **Type Hints** ì‚¬ìš©
- **Docstring** ì‘ì„±
- **ëª…í™•í•œ ë³€ìˆ˜ëª…** ì‚¬ìš©

#### ì˜ˆì‹œ
```python
def process_chat_request(self, request: ChatRequest) -> ChatResponse:
    """
    ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜
    
    Args:
        request: ì±„íŒ… ìš”ì²­ ë°ì´í„°
        
    Returns:
        ChatResponse: ì±„íŒ… ì‘ë‹µ ë°ì´í„°
        
    Raises:
        ChatServiceException: ì±„íŒ… ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        ValidationException: ìš”ì²­ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ
    """
```

### 2. ì—ëŸ¬ ì²˜ë¦¬

#### ì˜ˆì™¸ ì²˜ë¦¬ ì›ì¹™
1. **êµ¬ì²´ì ì¸ ì˜ˆì™¸ ì‚¬ìš©**
2. **ì ì ˆí•œ ë¡œê¹…**
3. **ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€**
4. **ë””ë²„ê¹… ì •ë³´ í¬í•¨**

#### ì˜ˆì‹œ
```python
try:
    response = await self.openai_client.generate_response(messages)
except OpenAIClientException as e:
    logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    raise ChatServiceException(f"AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e.message}")
except Exception as e:
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    raise ChatServiceException(f"ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
```

### 3. ë¡œê¹…

#### ë¡œê¹… ë ˆë²¨
- **DEBUG**: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
- **INFO**: ì¼ë°˜ì ì¸ ì •ë³´
- **WARNING**: ê²½ê³  ë©”ì‹œì§€
- **ERROR**: ì˜¤ë¥˜ ë©”ì‹œì§€
- **CRITICAL**: ì‹¬ê°í•œ ì˜¤ë¥˜

#### ì˜ˆì‹œ
```python
logger.info(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹œì‘: {request.user_message[:50]}...")
logger.debug(f"OpenAI API ìš”ì²­ ì‹œì‘ (model: {model}, temperature: {temperature})")
logger.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
```

### 4. í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ êµ¬ì¡°
```
tests/
â”œâ”€â”€ test_unit.py          # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_scenarios.py     # ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
â””â”€â”€ test_input.py         # ì…ë ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
```

#### í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
```python
def test_chat_service_process_chat_request():
    """ì±„íŒ… ì„œë¹„ìŠ¤ ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    service = ChatService()
    request = ChatRequest(
        user_message="í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€",
        session_id="test_session"
    )
    
    response = service.process_chat_request(request)
    
    assert response.success is True
    assert response.response_text is not None
    assert response.session_id == "test_session"
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ ê´€ë¦¬

#### ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
```python
import gc

# ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
gc.collect()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
process = psutil.Process()
memory_usage = process.memory_info().rss / 1024 / 1024
logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
```

### 2. ë¹„ë™ê¸° ì²˜ë¦¬

#### ë™ì‹œ ìš”ì²­ ì œí•œ
```python
import asyncio

# ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ ë™ì‹œ ìš”ì²­ ì œí•œ
semaphore = asyncio.Semaphore(10)

async def process_request(request):
    async with semaphore:
        return await chat_service.process_chat_request(request)
```

### 3. ìºì‹±

#### ì‘ë‹µ ìºì‹±
```python
import functools
import time

@functools.lru_cache(maxsize=1000)
def cache_response(key: str, ttl: int = 300):
    """ì‘ë‹µ ìºì‹±"""
    # ìºì‹œ ë¡œì§ êµ¬í˜„
    pass
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **[ê°œìš” ë° ì‹œì‘ ê°€ì´ë“œ](./../overview/README.md)**: í”„ë¡œì íŠ¸ ì†Œê°œ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
- **[API ë¬¸ì„œ](./../api/README.md)**: API ëª…ì„¸ ë° ì‚¬ìš©ë²•
- **[ë°°í¬ ê°€ì´ë“œ](./../deployment/README.md)**: ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ
- **[í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](./../testing/README.md)**: í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ ê´€ë¦¬

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ì‘ì„±ì**: LLM Server ê°œë°œíŒ€ 