# LLM Server ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” LLM Serverë¥¼ ë‹¤ì–‘í•œ í™˜ê²½ì— ë°°í¬í•˜ê³  ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸš€ ë°°í¬ ê°œìš”

### ì§€ì› ë°°í¬ ë°©ì‹

1. **ë¡œì»¬ ì‹¤í–‰** - ê°œë°œ ë° í…ŒìŠ¤íŠ¸ìš©
2. **Docker ì»¨í…Œì´ë„ˆ** - ê¶Œì¥ ë°°í¬ ë°©ì‹
3. **Docker Compose** - ë‹¤ì¤‘ ì„œë¹„ìŠ¤ ë°°í¬
4. **í´ë¼ìš°ë“œ ë°°í¬** - AWS, GCP, Azure ë“±

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **Python**: 3.11 ì´ìƒ
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 512MB (ê¶Œì¥ 1GB ì´ìƒ)
- **ë””ìŠ¤í¬**: ìµœì†Œ 100MB ì—¬ìœ  ê³µê°„
- **ë„¤íŠ¸ì›Œí¬**: OpenAI API ì ‘ê·¼ ê°€ëŠ¥

---

## ğŸ“¦ ë¡œì»¬ ë°°í¬

### 1. í™˜ê²½ ì„¤ì •

#### Python ê°€ìƒí™˜ê²½ ìƒì„±
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

#### íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirement.txt

# ê°œë°œìš© íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
pip install pytest black flake8
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

#### `.env` íŒŒì¼ ìƒì„±
```env
# ì„œë²„ ì„¤ì •
SERVER_HOST=0.0.0.0
SERVER_PORT=5601

# OpenAI API ì„¤ì •
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# ë¡œê¹… ì„¤ì •
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
```

#### í™˜ê²½ ë³€ìˆ˜ ì§ì ‘ ì„¤ì •
```bash
# Windows
set OPENAI_API_KEY=your_openai_api_key_here
set SERVER_HOST=0.0.0.0
set SERVER_PORT=5601

# Linux/Mac
export OPENAI_API_KEY=your_openai_api_key_here
export SERVER_HOST=0.0.0.0
export SERVER_PORT=5601
```

### 3. ì„œë²„ ì‹¤í–‰

#### ê¸°ë³¸ ì‹¤í–‰
```bash
python app.py
```

#### ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Linux/Mac)
```bash
nohup python app.py > app.log 2>&1 &
```

#### PM2ë¥¼ ì‚¬ìš©í•œ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ (Node.js í•„ìš”)
```bash
# PM2 ì„¤ì¹˜
npm install -g pm2

# ì„œë²„ ì‹¤í–‰
pm2 start app.py --name "llm-server" --interpreter python

# ìƒíƒœ í™•ì¸
pm2 status

# ë¡œê·¸ í™•ì¸
pm2 logs llm-server

# ì„œë²„ ì¤‘ì§€
pm2 stop llm-server
```

### 4. ì„œë²„ í™•ì¸

```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:5601/

# API ë¬¸ì„œ í™•ì¸
curl http://localhost:5601/docs

# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
curl http://localhost:5601/api/v1/system/status
```

---

## ğŸ³ Docker ë°°í¬

### 1. Docker ì„¤ì¹˜

#### Windows
- Docker Desktop for Windows ì„¤ì¹˜
- WSL2 í™œì„±í™” (ê¶Œì¥)

#### Linux (Ubuntu)
```bash
# Docker ì„¤ì¹˜
sudo apt-get update
sudo apt-get install docker.io docker-compose

# Docker ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start docker
sudo systemctl enable docker

# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER
```

#### macOS
- Docker Desktop for Mac ì„¤ì¹˜

### 2. Docker ì´ë¯¸ì§€ ë¹Œë“œ

#### ê¸°ë³¸ ë¹Œë“œ
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t llm-server .

# ì´ë¯¸ì§€ í™•ì¸
docker images
```

#### íƒœê·¸ ì§€ì • ë¹Œë“œ
```bash
# ë²„ì „ íƒœê·¸ ì§€ì •
docker build -t llm-server:v1.0 .

# ìµœì‹  íƒœê·¸ ì¶”ê°€
docker tag llm-server:v1.0 llm-server:latest
```

### 3. Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰

#### ê¸°ë³¸ ì‹¤í–‰
```bash
# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name llm-server \
  -p 5601:5601 \
  -e OPENAI_API_KEY=your-api-key \
  llm-server

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps

# ë¡œê·¸ í™•ì¸
docker logs llm-server
```

#### í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ì‚¬ìš©
```bash
# .env íŒŒì¼ ìƒì„±
echo "OPENAI_API_KEY=your-api-key" > .env
echo "SERVER_HOST=0.0.0.0" >> .env
echo "SERVER_PORT=5601" >> .env

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ë¡œ ì‹¤í–‰
docker run -d \
  --name llm-server \
  -p 5601:5601 \
  --env-file .env \
  llm-server
```

#### ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ë¡œê·¸ ë° ì„¤ì •)
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs

# ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ì‹¤í–‰
docker run -d \
  --name llm-server \
  -p 5601:5601 \
  -v $(pwd)/logs:/app/logs \
  -v $(pwd)/.env:/app/.env \
  -e OPENAI_API_KEY=your-api-key \
  llm-server
```

### 4. Docker Compose ì‚¬ìš©

#### `docker-compose.yml` ì„¤ì •
```yaml
version: '3.8'

services:
  llm-server:
    build: .
    container_name: llm-server
    ports:
      - "5601:5601"
    env_file:
      - .env
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/"]
      interval: 30s
      timeout: 10s
      retries: 3
```

#### Docker Compose ì‹¤í–‰
```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose down
```

#### í”„ë¡œë•ì…˜ìš© Docker Compose
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  llm-server:
    build: .
    container_name: llm-server-prod
    ports:
      - "5601:5601"
    env_file:
      - .env
    volumes:
      - ./logs:/app/logs
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge
```

---

## â˜ï¸ í´ë¼ìš°ë“œ ë°°í¬

### 1. AWS ë°°í¬

#### EC2 ì¸ìŠ¤í„´ìŠ¤ ë°°í¬
```bash
# EC2 ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ì†
ssh -i your-key.pem ubuntu@your-ec2-ip

# Docker ì„¤ì¹˜
sudo apt-get update
sudo apt-get install docker.io docker-compose

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-repo/llm-server.git
cd llm-server

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "OPENAI_API_KEY=your-api-key" > .env

# Docker Compose ì‹¤í–‰
sudo docker-compose up -d
```

#### ECS ë°°í¬
```yaml
# task-definition.json
{
  "family": "llm-server",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "executionRoleArn": "arn:aws:iam::account:role/ecsTaskExecutionRole",
  "containerDefinitions": [
    {
      "name": "llm-server",
      "image": "your-account.dkr.ecr.region.amazonaws.com/llm-server:latest",
      "portMappings": [
        {
          "containerPort": 5601,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "OPENAI_API_KEY",
          "value": "your-api-key"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/llm-server",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      }
    }
  ]
}
```

### 2. Google Cloud Platform ë°°í¬

#### Cloud Run ë°°í¬
```bash
# í”„ë¡œì íŠ¸ ì„¤ì •
gcloud config set project your-project-id

# Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ
docker build -t gcr.io/your-project-id/llm-server .
docker push gcr.io/your-project-id/llm-server

# Cloud Run ë°°í¬
gcloud run deploy llm-server \
  --image gcr.io/your-project-id/llm-server \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars OPENAI_API_KEY=your-api-key
```

### 3. Azure ë°°í¬

#### Azure Container Instances
```bash
# Azure CLI ë¡œê·¸ì¸
az login

# ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ ìƒì„±
az group create --name llm-server-rg --location eastus

# ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤ ë°°í¬
az container create \
  --resource-group llm-server-rg \
  --name llm-server \
  --image your-registry.azurecr.io/llm-server:latest \
  --dns-name-label llm-server \
  --ports 5601 \
  --environment-variables OPENAI_API_KEY=your-api-key
```

---

## ğŸ”§ ìš´ì˜ ê´€ë¦¬

### 1. ëª¨ë‹ˆí„°ë§

#### ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
```bash
# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
curl http://localhost:5601/api/v1/system/info

# í—¬ìŠ¤ì²´í¬
curl http://localhost:5601/api/v1/system/status

# CPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
watch -n 5 'curl -s http://localhost:5601/api/v1/system/cpu | jq ".cpu.usage_percent"'

# ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
curl http://localhost:5601/api/v1/system/memory

# ë””ìŠ¤í¬ ì •ë³´ í™•ì¸
curl http://localhost:5601/api/v1/system/disk
```

#### ë¡œê·¸ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f logs/app.log

# Docker ë¡œê·¸ í™•ì¸
docker logs -f llm-server

# Docker Compose ë¡œê·¸ í™•ì¸
docker-compose logs -f
```

#### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats llm-server

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
netstat -tulpn | grep 5601

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep python
```

### 2. ë°±ì—… ë° ë³µêµ¬

#### ì„¤ì • ë°±ì—…
```bash
# ì„¤ì • íŒŒì¼ ë°±ì—…
cp .env .env.backup.$(date +%Y%m%d)

# Docker ì´ë¯¸ì§€ ë°±ì—…
docker save llm-server > llm-server-backup.tar

# ë¡œê·¸ ë°±ì—…
tar -czf logs-backup-$(date +%Y%m%d).tar.gz logs/
```

#### ë°ì´í„° ë³µêµ¬
```bash
# ì„¤ì • ë³µêµ¬
cp .env.backup.20241201 .env

# Docker ì´ë¯¸ì§€ ë³µêµ¬
docker load < llm-server-backup.tar

# ë¡œê·¸ ë³µêµ¬
tar -xzf logs-backup-20241201.tar.gz
```

### 3. ì—…ë°ì´íŠ¸ ë° ìœ ì§€ë³´ìˆ˜

#### ì• í”Œë¦¬ì¼€ì´ì…˜ ì—…ë°ì´íŠ¸
```bash
# ì½”ë“œ ì—…ë°ì´íŠ¸
git pull origin main

# Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ
docker-compose build

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose up -d

# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
docker-compose down
docker tag llm-server:previous llm-server:latest
docker-compose up -d
```

#### ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
```bash
# Python íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
pip install --upgrade -r requirement.txt

# Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ
docker build --no-cache -t llm-server .
```

### 4. ë³´ì•ˆ ê´€ë¦¬

#### í™˜ê²½ ë³€ìˆ˜ ë³´ì•ˆ
```bash
# ë¯¼ê°í•œ ì •ë³´ ì•”í˜¸í™”
echo "OPENAI_API_KEY=$(echo -n 'your-api-key' | base64)" >> .env

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ê¶Œí•œ ì„¤ì •
chmod 600 .env

# Docker secrets ì‚¬ìš© (Swarm ëª¨ë“œ)
echo "your-api-key" | docker secret create openai_api_key -
```

#### ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ
```bash
# ë°©í™”ë²½ ì„¤ì •
sudo ufw allow 5601/tcp

# Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬
docker network create --driver bridge llm-network
docker run --network llm-network llm-server
```

---

## ğŸš¨ ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œ

#### ì„œë²„ ì‹œì‘ ì‹¤íŒ¨
```bash
# í¬íŠ¸ ì¶©ëŒ í™•ì¸
netstat -tulpn | grep 5601

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 $(lsof -t -i:5601)

# Docker ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart llm-server
```

#### OpenAI API ì—°ê²° ì‹¤íŒ¨
```bash
# API Key í™•ì¸
echo $OPENAI_API_KEY

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
curl -I https://api.openai.com/v1/models

# í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h

# Docker ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run --memory=1g llm-server

# ë¶ˆí•„ìš”í•œ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker system prune -a
```

### 2. ë¡œê·¸ ë¶„ì„

#### ì˜¤ë¥˜ ë¡œê·¸ ê²€ìƒ‰
```bash
# ì˜¤ë¥˜ ë¡œê·¸ í™•ì¸
grep "ERROR" logs/app.log

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸ í™•ì¸
grep "2024-12-01" logs/app.log

# ì‹¤ì‹œê°„ ì˜¤ë¥˜ ëª¨ë‹ˆí„°ë§
tail -f logs/app.log | grep "ERROR"
```

#### ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„
```bash
# ì‘ë‹µ ì‹œê°„ ë¶„ì„
grep "response_time" logs/app.log | awk '{print $NF}' | sort -n

# í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„
grep "total_tokens_used" logs/app.log | awk '{sum+=$NF} END {print sum}'
```

### 3. ë””ë²„ê¹…

#### ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰
```bash
# ë¡œê·¸ ë ˆë²¨ ë³€ê²½
export LOG_LEVEL=DEBUG

# ìƒì„¸ ë¡œê·¸ í™•ì¸
docker-compose logs -f --tail=100
```

#### API í…ŒìŠ¤íŠ¸
```bash
# ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:5601/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{"user_message": "í…ŒìŠ¤íŠ¸"}'

# ìƒì„¸ ì‘ë‹µ í™•ì¸
curl -v -X POST "http://localhost:5601/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{"user_message": "í…ŒìŠ¤íŠ¸"}'
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ë¦¬ì†ŒìŠ¤ ìµœì í™”

#### ë©”ëª¨ë¦¬ ìµœì í™”
```python
# app.pyì—ì„œ ë©”ëª¨ë¦¬ ì„¤ì •
import gc

# ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
gc.collect()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
process = psutil.Process()
print(f"Memory usage: {process.memory_info().rss / 1024 / 1024} MB")
```

#### CPU ìµœì í™”
```python
# ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
import asyncio

# ë™ì‹œ ìš”ì²­ ì œí•œ
semaphore = asyncio.Semaphore(10)

async def process_request(request):
    async with semaphore:
        return await chat_service.process_chat(request)
```

### 2. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

#### ì—°ê²° í’€ë§
```python
import aiohttp

# HTTP í´ë¼ì´ì–¸íŠ¸ í’€ ì„¤ì •
async with aiohttp.ClientSession(
    connector=aiohttp.TCPConnector(
        limit=100,
        limit_per_host=30,
        ttl_dns_cache=300
    )
) as session:
    # API í˜¸ì¶œ
    pass
```

#### ìºì‹± ì „ëµ
```python
import functools
import time

# ì‘ë‹µ ìºì‹±
@functools.lru_cache(maxsize=1000)
def cache_response(key, ttl=300):
    # ìºì‹œ ë¡œì§
    pass
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **[ê°œìš” ë° ì‹œì‘ ê°€ì´ë“œ](./../overview/README.md)**: í”„ë¡œì íŠ¸ ì†Œê°œ ë° ê¸°ë³¸ ì‚¬ìš©ë²•
- **[API ë¬¸ì„œ](./../api/README.md)**: API ëª…ì„¸ ë° ì‚¬ìš©ë²•
- **[ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](./../architecture/README.md)**: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ê°œë°œ ê°€ì´ë“œ
- **[í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](./../testing/README.md)**: í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ ê´€ë¦¬

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ì‘ì„±ì**: LLM Server ê°œë°œíŒ€ 